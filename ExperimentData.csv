SLR_Author,AbstractID,Abstract_Year,Abstract
Luyi Feng,1,2017,"Despite the importance of conducting systematic literature reviews (SLRs) for identifying the research gaps in software engineering (SE) research, SLRs are a complex, multi-stage, and time-consuming process if performed manually. Conducting an SLR in line with the guidelines and practice in the SE domain requires considerable effort and expertise. The objective of this SLR is to identify and classify text-mining techniques and tools that can help facilitate SLR activities. This study also investigates the adoption of text-mining (TM) techniques to support SLR in the SE domain. We performed a mixed search strategy to identify relevant studies published from January 1, 2004, to December 31, 2016. We shortlisted 32 papers into the final set of relevant studies published in the SE, medicine and social science disciplines. The majority of the text-mining techniques attempted to support the study selection stage. Only 12 out of the 14 studies in the SE domain applied text-mining techniques, focusing primarily on facilitating the search and study selection stages. By learning from the experience of applying TM techniques in clinical medicine and social science fields, we believe that SE researchers can adopt appropriate SLR automation strategies for use in the SE field."
Omara,2,2015,"The large and growing number of published studies, and their increasing rate of publication, makes the task of identifying relevant studies in an unbiased way for inclusion in systematic reviews both complex and time consuming. Text mining has been offered as a potential solution: through automating some of the screening process, reviewer time can be saved. The evidence base around the use of text mining for screening has not yet been pulled together systematically; this systematic review fills that research gap. Focusing mainly on non-technical issues, the review aims to increase awareness of the potential of these technologies and promote further collaborative research between the computer science and systematic review communities."
SR Jonnalagadda,3,2015,"Background: Automation of the parts of systematic review process, specifically the data extraction step, may be an important strategy to reduce the time necessary to complete a systematic review. However, the state of the science of automatically extracting data elements from full texts has not been well described. This paper performs a systematic review of published and unpublished methods to automate data extraction for systematic reviews.
Methods :We systematically searched PubMed, IEEEXplore, and ACM Digital Library to identify potentially relevant articles. We included reports that met the following criteria: 1) methods or results section described what entities were or need to be extracted, and 2) at least one entity was automatically extracted with evaluation results that were presented for that entity. We also reviewed the citations from included reports.
Results :Out of a total of 1190 unique citations that met our search criteria, we found 26 published reports describing automatic extraction of at least one of more than 52 potential data elements used in systematic reviews. For 25 (48 %) of the data elements used in systematic reviews, there were attempts from various researchers to extract information automatically from the publication text. Out of these, 14 (27 %) data elements were completely extracted, but the highest number of data elements extracted automatically by a single study was 7. Most of the data elements were extracted with F-scores (a mean of sensitivity and positive predictive value) of over 70 %.
Conclusions :We found no unified information extraction framework tailored to the systematic review process, and published reports focused on a limited (1 to 7) number of data elements. Biomedical natural language processing techniques have not been fully utilized to fully or even partially automate the data extraction step of systematic reviews."
Al-Zubidy,4,2015,"Context :The number of published systematic literature reviews (SLRs) in software engineering venues is increasing. However, even with their high adaptation rate, the task of performing an SLR requires a large amount of effort and presents a number of barriers. Specifically, during the SLR search phase authors must expend a lot of time and overcome a large number of barriers.
Objective:To help alleviate some of the barriers in the search phase, we identify and prioritize SLR search tool requirements based on input from the SLR community. These requirements will help tool builders ensure they focus their efforts appropriately.
Method :We conducted an SLR and a survey of SLR authors in software engineering. In the SLR we extracted problems and solutions SLR authors reported during their search processes. In the survey we asked respondents to describe the problems they faced during SLR search and to specify any requirements they would like to see as part of an SLR search tool. We also asked survey respondents to describe any tools they use to support SLR search, along with the strengths and limitations of those tools.
Results :Based on analysis of 84 studies from the literature and a qualitative analysis of 131 responses from researchers, we identified a set of functional requirements that authors, researchers, and tool builders can use as a reference. We also describe the tools currently used by SLR researchers.
Conclusions :The list of SLR search tool requirements can be used by tool builders as a guide when constructing new tools. Our analysis of tools showed that more recent tools are covering more of the requirements than in the past."
Al-Zubidy,5,2018,"Background: Systematic literature reviews (SLRs)
have become an established methodology in software engineering
(SE) research however they can be very time consuming and
error prone. Aim: The aims of this study are to identify and
classify tools that can help to automate part or all of the SLR
process within the SE domain. Method: A mapping study was
performed using an automated search strategy plus snowballing
to locate relevant papers. A set of known papers was used to
validate the search string. Results: 14 papers were accepted into
the final set. Eight presented text mining tools and six discussed
the use of visualisation techniques. The stage most commonly
targeted was study selection. Only two papers reported an
independent evaluation of the tool presented. The majority were
evaluated through small experiments and examples of their use.
Conclusions: A variety of tools are available to support the SLR
process although many are in the early stages of development and
usage. "
Felizardo,6,2015,"A systematic literature review (SLR) is a methodology used to find and aggregate all relevant existing evidence about a specific research question of interest. Important decisions need to be made at several points in the review process, relating to search of the literature, selection of relevant primary studies and use of methods of synthesis. Visualization can support tasks that involve large collections of data, such as the studies collected, evaluated and summarized in an SLR. The objective of this paper is to present the results of a systematic mapping study (SM) conducted to collect and evaluate evidence on the use of a specific visualization technique, visual data mining (VDM), to support the SLR process. We reviewed 20 papers and our results indicate a scarcity of research on the use of VDM to help with conducting SLRs in the software engineering domain. However, most of the studies (16 of the 20 studies included in our mapping) have been conducted in the field of medicine and they revealed that the activities of data extraction and data synthesis, related to conducting the review phase of an SLR process, have more VDM support than other activities. In contrast, according to our SM, previous studies using VDM techniques with SLRs have not employed such techniques during the SLR s planning and reporting phases"
Tsafnat,7,2014,"Systematic reviews, a cornerstone of evidence-based medicine, are not produced quickly enough to support clinical practice. The cost of production, availability of the requisite expertise and timeliness are often quoted as major contributors for the delay. This detailed survey of the state of the art of information systems designed to support or automate individual tasks in the systematic review, and in particular systematic reviews of randomized controlled clinical trials, reveals trends that see the convergence of several parallel research projects.

We surveyed literature describing informatics systems that support or automate the processes of systematic review or each of the tasks of the systematic review. Several projects focus on automating, simplifying and/or streamlining specific tasks of the systematic review. Some tasks are already fully automated while others are still largely manual. In this review, we describe each task and the effect that its automation would have on the entire systematic review process, summarize the existing information system support for each task, and highlight where further research is needed for realizing automation for the task. Integration of the systems that automate systematic review tasks may lead to a revised systematic review workflow. We envisage the optimized workflow will lead to system in which each systematic review is described as a computer program that automatically retrieves relevant trials, appraises them, extracts and synthesizes data, evaluates the risk of bias, performs meta-analysis calculations, and produces a report in real time."
Marshal,8,2018,"Context: Previously, the authors had developed and evaluated a framework to evaluate systematic review (SR) lifecycle tools. Goal: The goal of this study was to use the experiences of researchers in other domains to further evaluate and refine the evaluation framework. Method: We investigate the opinions of researchers with experience of systematic reviews in the healthcare and social sciences domains. We used semi-structured interviews to elicit their experiences of systematic reviews and SR support tools. Results: Study participants found broadly the same problems as software engineering (SE) researchers with the SR process. They agreed with the tool features we had included in our evaluation framework. Furthermore although there were some differences the majority of the importance assessments were very close. Conclusions: In the context of SRs, experiences of researchers in other domains can be useful to software engineering researchers. The evaluation framework for SR lifecycle tools appeared quite robust."
"Olorisade

",9,2016,"Background: Since the introduction of the systematic review process to Software Engineering in 2004, researchers have investigated a number of ways to mitigate the amount of effort and time taken to filter through large volumes of literature.

Aim: This study aims to provide a critical analysis of text mining techniques used to support the citation screening stage of the systematic review process.

Method: We critically re-reviewed papers included in a previous systematic review which addressed the use of text mining methods to support the screening of papers for inclusion in a review. The previous review did not provide a detailed analysis of the text mining methods used. We focus on the availability in the papers of information about the text mining methods employed, including the description and explanation of the methods, parameter settings, assessment of the appropriateness of their application given the size and dimensionality of the data used, performance on training, testing and validation data sets, and further information that may support the reproducibility of the included studies.

Results: Support Vector Machines (SVM), Naïve Bayes (NB) and Committee of classifiers (Ensemble) are the most used classification algorithms. In all of the studies, features were represented with Bag-of-Words (BOW) using both binary features (28%) and term frequency (66%). Five studies experimented with n-grams with n between 2 and 4, but mostly the unigram was used. ?2, information gain and tf-idf were the most commonly used feature selection techniques. Feature extraction was rarely used although LDA and topic modelling were used. Recall, precision, F and AUC were the most used metrics and cross validation was also well used. More than half of the studies used a corpus size of below 1,000 documents for their experiments while corpus size for around 80% of the studies was 3,000 or fewer documents. The major common ground we found for comparing performance assessment based on independent replication of studies was the use of the same dataset but a sound performance comparison could not be established because the studies had little else in common. In most of the studies, insufficient information was reported to enable independent replication. The studies analysed generally did not include any discussion of the statistical appropriateness of the text mining method that they applied. In the case of applications of SVM, none of the studies report the number of support vectors that they found to indicate the complexity of the prediction engine that they use, making it impossible to judge the extent to which over-fitting might account for the good performance results.

Conclusions: There is yet to be concrete evidence about the effectiveness of text mining algorithms regarding their use in the automation of citation screening in systematic reviews. The studies indicate that options are still being explored, but there is a need for better reporting as well as more explicit process details and access to datasets to facilitate study replication for evidence strengthening. In general, the reader often gets the impression that text mining algorithms were applied as magic tools in the reviewed papers, relying on default settings or default optimization of available machine learning toolboxes without an in-depth understanding of the statistical validity and appropriateness of such tools for text mining purposes."
Thomas,10,2014,"Systematic reviews are a widely accepted research method. However, it is increasingly difficult to conduct them to fit with policy and practice timescales, particularly in areas which do not have well indexed, comprehensive bibliographic databases. Text mining technologies offer one possible way forward in reducing the amount of time systematic reviews take to conduct. They can facilitate the identification of relevant literature, its rapid description or categorization, and its summarization.

In this paper, we describe the application of four text mining technologies, namely, automatic term recognition, document clustering, classification and summarization, which support the identification of relevant studies in systematic reviews. The contributions of text mining technologies to improve reviewing efficiency are considered and their strengths and weaknesses explored.

We conclude that these technologies do have the potential to assist at various stages of the review process. However, they are relatively unknown in the systematic reviewing community, and substantial evaluation and methods development are required before their possible impact can be fully assessed. Copyright © 2011 John Wiley & Sons, Ltd."
Kitchenham and P. Brereton,11,2013,"Context
Many researchers adopting systematic reviews (SRs) have also published papers discussing problems with the SR methodology and suggestions for improving it. Since guidelines for SRs in software engineering (SE) were last updated in 2007, we believe it is time to investigate whether the guidelines need to be amended in the light of recent research.

Objective
To identify, evaluate and synthesize research published by software engineering researchers concerning their experiences of performing SRs and their proposals for improving the SR process.

Method
We undertook a systematic review of papers reporting experiences of undertaking SRs and/or discussing techniques that could be used to improve the SR process. Studies were classified with respect to the stage in the SR process they addressed, whether they related to education or problems faced by novices and whether they proposed the use of textual analysis tools.

Results
We identified 68 papers reporting 63 unique studies published in SE conferences and journals between 2005 and mid-2012. The most common criticisms of SRs were that they take a long time, that SE digital libraries are not appropriate for broad literature searches and that assessing the quality of empirical studies of different types is difficult.

Conclusion
We recommend removing advice to use structured questions to construct search strings and including advice to use a quasi-gold standard based on a limited manual search to assist the construction of search stings and evaluation of the search process. Textual analysis tools are likely to be useful for inclusion/exclusion decisions and search string construction but require more stringent evaluation. SE researchers would benefit from tools to manage the SR process but existing tools need independent validation. Quality assessment of studies using a variety of empirical methods remains a major problem."
a,100,2019,"Humans have always attempted to correspond with objects in a natural language. Communications have been the essential feature of human life, a powerful tool for sharing and building the information that is passed from generation to generation. Among speech processing problems, automatic speech recognition mechanisms of converting the recorded speech signals into the text are one of the most challenging tasks. The signals are typically processed in a digital representation, so speech processing can be observed as a particular case of digital signal processing. The overall performance of an automatic speech recognition system greatly depends upon the acoustic modeling. Hence, building a precise and robust acoustic model holds the key to a suitable recognition performance. People have used different methods for automated speech recognition system. For recognizing the speech people always choose the English language in the majority of the research and implementation but very less work is done in other languages. Our analysis presents the study of the different speech recognition systems present in Indian and foreign languages in the systematic review of speech recognition paper. This paper gives the review of different aspects related to Automatic Speech recognition. We have elaborated the recent advancement in the speech recognition system, robust method for the development of an automatic speech recognition system and application of automatic speech recognition system in different fields."
b,101,2018,"The incorporation of Information and Communication Technology has changed the landscape of K-12 education, contributing to immense improvement and increased expectations within the sector. Following the introduction of virtual learning environments for use by higher education institutions and the resulting refinement in their design and application, virtual schools are now being applied to K-12 education and have become one of the most transformational trends in such education. This paper reviews the literature to provide insight into the critical distinctions between the terms and definitions used for various aspects of virtual school. In conducting this review, a distinct lack of framework on the critical factors necessary for success as regards the implementation of virtual schools was found in the current literature. To overcome this lack, this paper presents a conceptual framework derived from an in-depth review of related literature. The result of the systematic review revealed 15 factors which have been categorized under five dimensions; namely, technology (system quality, system interactivity, ease of use and usefulness), student's characteristics (self-efficacy and attitude), instructor's characteristics (control of technology, attitude and teaching style), course and content (content quality, diversity of assessment methods and course flexibility) and institutional support (technical support, user training and top management support). This conceptual framework offers the main frame of reference and potential lines of investigation that could be used by educational institutions that seek to implement and develop projects which incorporate eLearning as a main component."
c,102,2020,"General Systems Theory (GST), as it was introduced by Karl Ludwig von Bertalannfy, had a significant, yet mostly unacknowledged influence on systems theory. The purpose of the current review is to highlight and assess the application of the aforementioned theory in the healthcare field and suggest a new approach to the public hearing healthcare sector. A systematic literature review has been conducted in the electronic databases of ScienceDirect, Pubmed and IEEEXplore covering the years of 2009-2019, following the PRISMA guidelines. The article selection was performed to identify GST-related frameworks in the healthcare field and was completed through a process of removing duplicates and non-available articles, analyzing the tittle and abstract, and then reviewing the full text of each selected article. In the final analysis, 47 studies were selected and were thoroughly analysed. Almost half of these articles showed a practical implementation of GST-inspired frameworks, following different types of research methodology. Analysis of these methodologies identified the limitations and positive effects of GST in the healthcare field. Although there is a significant number of references in GST in the healthcare field over the last 10 years, applications of it need to be further tested and explored before they are put into real-situation testing. Simulation models and evidence-based approaches on a micro-, meso- and macro-level of systems should be used to provide the contextual information needed for establishing GST as a driving force in the healthcare field. To this context, a paradigm of GST data framework applied to the hearing loss screening area is hereby presented and discussed. It is shown that a GST approach can be used to identify equilibria in all levels, to balance gain and prediction capacity over time and enhance public hearing health approaches for treatment and management strategies."
d,103,2017,"Different social and psychological variables has been identified in previous studies leading to understand the tourist's behavior and intentions to visit a destination. However, tourism literature does not have a combined list of these constructs for predicting such intentions. Understanding these constructs can provide guidelines for tourism's stakeholders to attract tourists to a specific destination. This review identifies the variables that provoke the intentions of tourists to visit a destination. A systematic literature review methodology has been adopted, focusing on conference papers published during the last six year on tourism research. The study uncovered six essential constructs, namely: Satisfaction, Destination Attractions, Perceived Behavioral Control (PBC), Perceived Risk, Perceived Value, and Experience. A metamodel has been designed revealing the relationship among the aforementioned constructs. Moreover, the findings show that tourist's satisfaction, experience and destination attractions creates positive intentions to visit a destination or recommend it to others."
a,104,2017,"A systematic review about a particular subject provides the basis of knowledge for supporting a research. The increase of scientific information available and the easy access to such information in electronic databases have contributed to the rise of systematic review studies. However, one of the problems that arise with traditional research methods is the difficulty of reading all available articles. Thus, some items must be selected according to predefined selection rules. Nevertheless, the effectiveness of a systematic review is directly related to the relevance of the scientific papers selected according to the purpose of the particular study, among others factors. Therefore, there are several indicators that can be used to prioritize the articles that will set the articles portfolio of the review. The objective of this article is to introduce a method and a web system, implemented in R package, designed to apply automated filters to help in the selection of articles for systematic reviews. We propose two filters in the discussed method: the journal impact factor and the number of citations of the articles. The latter was analyzed by using Pareto rule. In the portfolio creation process, articles are first selected from a query performed in the electronic databases Scopus and Web of Science. Then, the proposed method can be automatically applied using the web system introduced here."
b,105,2018,"Educational Robotics has been presented as a great pedagogical tool because it demonstrates an attractive way of working the theoretical knowledge put into practice. Thus, several educational technologies have emerged with different approaches, with the purpose of applying robotics in the educational area in a more attractive and playful way. This article presents the conduction of a Systematic Review of Literature (SRL), whose objective is to identify the teaching approaches used with educational robotics. With this, we present experiences reports, and at the same time show the skills and competencies that are explored through robotics and education. This review uses scientific papers published in the period from 2011 to 2016."
c,106,2019,"Autonomous Vehicles (AV) are expected to bring considerable benefits to society, such as traffic optimization and accidents reduction. They rely heavily on advances in many Artificial Intelligence (AI) approaches and techniques. However, while some researchers in this field believe AI is the core element to enhance safety, others believe AI imposes new challenges to assure the safety of these new AI-based systems and applications. In this non-convergent context, this paper presents a systematic literature review to paint a clear picture of the state of the art of the literature in AI on AV safety. Based on an initial sample of 4870 retrieved papers, 59 studies were selected as the result of the selection criteria detailed in the paper. The shortlisted studies were then mapped into six categories to answer the proposed research questions. An AV system model was proposed and applied to orient the discussions about the SLR findings. As a main result, we have reinforced our preliminary observation about the necessity of considering a serious safety agenda for the future studies on AI-based AV systems."
a,107,2018,"Every project implementation must involve its management process. Project management is a system that enables organizations to manage their projects. The development of information technology encourages the development of techniques and methodologies used in project management. This study aims to provide a detailed insight into the trends of project management with systematic literature review and has a particular focus on the project management platform. Therefore, it is necessary to understand the features, technology and architecture of the project management platform. Meta-analysis is needed to determine the optimal layer of project management platform architecture with principal component analysis techniques. Trends in the project management platform literature were analyzed from 2000 to 2018. The findings from the analysis highlighted some of the generic features that project management platforms should have such as schedule/monitoring management, process management, configuration/user management, document management, financial/cost management and resources management. By doing principal component analysis, the number of optimal layers in the project management platform is obtained four: application layer, service layer, data integration layer and management layer."
b,108,2017,"According to Administration of Aging (AoA), by the year 2060, the older people will represent among the 98 million of people of the total population. That is, much of this priority group will be involved with technology. Hence, the Massive Open Online Courses (MOOCs) are a good alternative for lifelong learning. Therefore, it should be taken into account the needs of older people according to their learning styles by using andragogical techniques, strategies and accessibility criteria. However, nowadays many courses are offered in different platforms, but they are not oriented to older people with special characteristics according to the age. Moreover, the methodologies reviewed do not take into account accessibility criteria and andragogical techniques and strategies for building MOOCs."
c,109,2019,"Diabetes Mellitus is a syndrome that occurs due to insulin deficiency but there is no absolute cure for diabetes. There are larger effects of long term diabetes on human body such as eyes as cataracts, kidney failures, nerve damaging, ulcers in feet and gangrene. Hence there is need to understand that, this disease can be prevented through normal physiological process for avoiding greater damage. Thermography is identified as a potential tool to predict and detect the disease progression in an earlier stage in automated way. Actually this type of imaging maps the superficial temperature from the human body and able to identify the disease complications accurately. This paper discusses the recent literature studies with different intervention and comparison of appropriate methods used in detection of diabetes mellitus accurately. The intervention features of different works have been carried out with suitable analysis in identifying the issues and problems associated in medical diagnosis of Diabetic foot neuropathy. This work summarizes the use of thermography in diagnosis process and defines the scope for further improvement in the related research work."
d,110,2018,"Nowadays, with the demand of web applications there is also an increase in the number of problems and crimes that demand an investigation that requires digital forensics techniques in order to manage web evidence. Although there are several studies that address cyber forensics, they are mainly oriented to manage evidence at server side, as far as we know, no systematic literature reviews have been reported on how cyber forensics is addressed at clients' side; considering the internationals standards. Thus, this paper presents a literature review about how cyber forensics is addressed at clients' side related to techniques of identification, collection, analysis, preservation and report of digital evidence. Also, a review of how standards are being used in cyber forensics focused on the client side. The aim of this study is to provide a background of relevant activities that are considered by investigators to handle potentially digital evidence from web environments, considering what internationals standards are solving for this purpose. Thus, a total of 37 studies have been selected and analyzed in this study. Moreover, this study provides important insights about the need to create methodologies aligned with formal standards that support the management of the evidence in an appropriate way."
a,111,2019,"Multi-Agent Systems became a powerful solution to model and solve problems in complex and dynamic environments. While research in this area grew exponentially before 2009, there is a need to understand the status quo of the field from 2009 to June 2017 in order to comprehend the general evolution. The results of a SLR related to Multi-Agent Systems, its applications and research gaps, following Kitchenham and Wholin guidelines are presented in this paper. From the analysis of279 papers (out of3522 candidates), our findings suggest that: a) there is a general decreasing trend of publications (but it is increasing for specific domains), b) only 15% of the papers portrayed a real case study, c) the top 20 were formed by 67 authors, d) the papers were mostly published in journals and conferences, e) there is no unified methodology or framework, f) the top 3 application domains were transport/traffic, healthcare/biology, and logistics/manufacturing, g) MAS interact with different disciplines like machine learning. Finally, the MAS community should work together to close the gaps and unify the field, bridging with other disciplines and industry."
b,112,2020,"Android malware has been in an increasing trend in recent years due to the pervasiveness of Android operating system. Android malware is installed and run on the smartphones without explicitly prompting the users or without the user's permission, and it poses great threats to users such as the leakage of personal information and advanced fraud. To address these threats, various techniques are proposed by researchers and practitioners. Static analysis is one of these techniques, which is widely applied to Android malware detection and can detect malware quickly and prohibit malware before installation. To provide a clarified overview of the latest work in Android malware detection using static analysis, we perform a systematic literature review by identifying 98 studies from January 2014 to March 2020. Based on the features of applications, we first divide static analysis in Android malware detection into four categories, which include Android characteristic-based method, opcode-based method, program graph-based method, and symbolic execution-based method. Then we assess the malware detection capability of static analysis, and we compare the performance of different models in Android malware detection by analyzing the results of empirical evidence. Finally, it is concluded that static analysis is effective to detect Android malware. Moreover, there is a preliminary result that neural network model outperforms the non-neural network model in Android malware detection. However, static analysis still faces many challenges. Thus, it is necessary to derive some novel techniques for improving Android malware detection based on the current research community. Moreover, it is essential to establish a unified platform that is used to evaluate the performance of a series of techniques in Android malware detection fairly."
c,113,2019,"INTRODUCTION: The Physics of Notations (PoN) is a theory for the design of cognitively effective visual notations, emphasizing the need for design grounded in objective and verifiable rationale. Although increasingly applied, no systematic analysis of PoN applications has yet been performed to assess the theory's efficacy in practice. OBJECTIVES: Our primary objective was to assess the scope and verifiability of PoN applications. METHOD: We performed a systematic literature review (SLR) of peer-reviewed PoN applications. We analyzed what visual notations have been evaluated and designed using the PoN, for what reasons, to what degree applications consider requirements of their notation's users, and how verifiable these applications are. RESULTS: Seventy PoN applications were analyzed. We found major differences between applications evaluating existing notations and applications designing new notations. Particularly, in the case of new notations, we found that most applications adopted the PoN with little critical thought towards it, rarely considered its suitability for a particular context, and typically treated and discussed the PoN with few, if any, verifiable details and data. CONCLUSION: The results warrant consideration for those applying the PoN to do so carefully, and show the need for additional means to guide designers in systematically applying the PoN."
d,114,2019,"The growth in use of Blockchain technology has enabled the development of applications in various industries. In particular, there has been an interest about using it in government to manage electronic public services, such as configuring and improving electronic government (e-government). However, the in-depth study of practical Blockchain solutions in the government sector is still under-researched despite the growing academic interest in this area. Therefore, this article presents a systematic literature review based on the current technology relating to Blockchain architectures applied by governments to public services."
a,115,2018,"Education is one of important program that has always been the goal of the country's strategy to advance the society. Technology is one of various aspect that support the education sector to improve its quality, such as business intelligence. Business intelligence is a technology that is developing today and can help in the decision-making process form large number of data or known as big data analytics. This study identifies the application of business intelligence to the education sector, especially in terms of the technology, contribution, and application using PRISMA. Technology of business intelligence application (Q1) is categorized into two, that are techniques and tools. For the first category, there are four kind of techniques. The techniques are data mining, viable system model (VSM), learning analytics, cloud computing, and behavioural analytics. For the other category, there are several tools that used to apply the business intelligence. The tools are Hadoop, Gephi, BigData by IBM, and web-based. Contributions of business intelligence application (Q2) are knowledge transfer, innovation, and evaluation. Domain application of business intelligence (Q3) are the result of the general form of applying business intelligence to various fields such as research, curriculum, assessment, behaviour analysis, student enrolment, and resource management."
b,116,2017,"Business process is set of activities that define how to achieve organizational goals and also produces outputs that deliver value to customers. Business process improvement is very important to make the organization better, more competitive and successful. In recent year, business process improvement has noticed by researchers. In this study, we will provide a systematic literature review from 2011 until 2017. We used Kitchenham and Charter method to conduct a systematic literature review. Data source such as IEEE, ScienceDirect, ProQuest and SpringerLink used to obtain literature. We obtained 39 papers from 67 papers to complete this research. This systematic literature review conducted to see the research trend on the topics of business process improvement from the last few years, approaches and methods used in business process improvement."
c,117,2018,"Although cloud computing is one of the most significant trends in information technology acquisition today, its adoption amongst the SMEs is still behind the larger conterparts. Additionally, among those that use, many face challenges to gain benefits as what is normally claimed. More research is needed to understand the issue. The purpose of this paper is to present the findings of a Systematic Literature Review (SLR) conducted related to cloud computing adoption among SMEs, particularly focusing on the post adoption stage. SLR method was employed as this method enable the review been done in a more comprehensive and rigorous manner. A total of 39 relevant articles were reviewed and the findings indicate that most past researches on cloud computing and SMEs focused on adoption, exploring factors that affect the adoption. Very few studies looked at the post adoption stage or the impacts of cloud computing on SMEs."
d,118,2020,"Computer-assisted surgical procedures have become a major part of the development of robotics and medicine. These treatments can offer many benefits, such as a shorter recovery time, and improved quality and accuracy of diagnoses. We reviewed computer support literature for surgical interventions included in top peer-reviewed journals and conferences. Based on the review, we identify areas which are ready for computer support in surgical interventions and show future research needs-"
a,119,2017,"The past two decades have seen a tremendous growth in innovation processes conceived under scarcity conditions with special focus on emerging markets and bottom of the pyramid (BOP) customers. However, evolving literature in this field has unfortunately resulted in a multitude of innovation approaches leading to terminology confusion and fragmented literature. Hence, this study is an attempt to systematically organize and synthesize the research on innovation approaches originated in, for or from emerging markets. An extensive systematic review of the existing literature is carried out to investigate the progress of prior research, and to use the insights to define future research pathways. This review is primarily based on the most frequently used innovation approaches, especially frugal innovation, jugaad, disruptive innovation, Gandhian innovation, catalytic innovation, indigenous innovation, bricolage, blowback innovation, trickle-up innovation, resource-constrained innovation, and BOP innovation. Our analysis finds growing standardization in terminology usage and increasing emphasis on  Å bottom-up and structured innovation approaches. De-emphasizing the role of technology transfers and spillovers from the West, the findings exhibit increasing applications of these innovations beyond emerging markets to wider markets. Our research results also shed light on the evolution of the topic and instigate further research explorations in the direction of analyzing the user adoption of these constraint-based innovations and understanding the influence of new technological advancements, such as the Internet, mobile telecommunications, and Web 2.0 on the innovation process, with a special focus on the service industry."
b,120,2019,"Personality trait recognition has an essential role in the job screening process. The psychologies perform the analysis based on the survey, the handwriting of the participant, or conduct the interview. The process takes a lot of time and money. Consequently, it inspires that researchers develop a tool to help the screening process faster. The current review papers lack an explanation of deep learning algorithms in personality trait recognition. So we perform this study to classify the latest deep learning algorithms in personality trait recognition. Using Preferred Reporting Items for Systematic reviews and MetaAnalyses Method (PRISMA), we collected 25 key papers which are from Scopus, IEEE, Science Direct, Emerald Insight, and ACM. In this paper, we focus on discussing the problems, deep learning methods, and open issue in personality trait recognition. Based on the finding, the primary problem in personality trait recognition is the complexity of the text and audiovisual data. It induces the low performance of personality trait recognition. In detail, the text-based trait recognition has lower accuracy than the visual and audio based methods. Therefore we have a bigger opportunity to improve the performance of text-based personality recognition than audiovisual based. Moreover, the real experiment in audiovisual trait recognition still in the early stages and it can be explored in more detail."
c,121,2018,"The systematic literature review (SLR) presented in this paper analyzes the relationships among dynamic capabilities, strategic foresight and organizational learning. The purpose of the SLR on dynamic capabilities (DC), strategic foresight (SF) and organizational learning (OL) is to advance the academic perspective on dynamic capabilities by analyzing how SF and OL integrate into the DC framework. This advancement is necessary since the pace of changes in an organization's environment is increasing, causing competitive pressure to rise. An organization needs to address such changes by adjusting existing capabilities or acquire new ones and thus strengthen their competitiveness. SF provides information and insights about future developments or changes of an organization's business environment. After which OL integrates the obtained information into the strategic planning process. It is important to include the concepts of SF and OL into the DC framework, because an organization without learning cannot integrate information into the organizational decision-making process. The results of the SLR present a triangle relation ship among DC, SF and OL that can help to operationalize the DC framework. A practical application of dynamic capabilities facilitates the long-term performance and sustainability of an organization."
d,122,2017,"Enterprise Information System (EIS) is organization system that can be used to integrate several functional fields of organization, such as planning, manufacturing, marketing, distribution and e-business that can be used to support some management function of an organization as well as expand the business process. Therefore, this paper analyses and classifies EIS components to identify what types of components is a need in implementing EIS based on previous studies. Components in this research are divided into 3, which are: architecture, integration, and networking. Through systematic literature review using the PRISMA method, 15 publications from 2 search databases were chosen in relation to EIS topics. The aim of this study is to know type of components that are needed for EIS implementation. Therefore, through this study, EIS can be applied by choosing either one of more component types that can make an EIS that is accurate and appropriate for the business process."
a,123,2017,"Current research in design and development of an environmental management information system (EMIS) are urgently required which was triggered by environmental degradation caused by various factors including regional development. Managing environmental is required in developing such region as it very important to preserve environmental and protected species habitat. The main goal of developing EMIS is how to manage environmental efficiently and effectively by adopting Information and Communication Technologies (ICT) for environmental management. This systematic literature review is aimed to develop a better understanding regarding the aspects between January 2000 and February 2017. The method used in this literature review is PRISMA statement conducting systematically by selecting scientific paper related to the topic from accredited journal. After inclusion and exclusion process, a total number 41 scientific papers were obtained. The information regarding research trends, datasets, and methods were identified and analyzed from the selected papers. Identification of research gaps and challenges of further research were also explored."
b,124,2020,"Fraud is a primary source of organization losses, amounting to up to 5% of yearly revenues. Process-based fraud (PBF) is fraud involving a deviation from the standard operating procedure (SOP) of business processes. PBF hinders the achievement of business objectives because business processes operationalize organizational strategies. A systematic content analysis of the literature was conducted on fraud detection metrics in business processes. The current state of fraud detection was surveyed by focusing on PBF metrics while including all relevant conceptual perspectives of PBF detection. The findings indicate that a large body of research has examined detection metrics for possible fraud, but less attention has been paid to PBF. In addition, the currently available PBF detection metrics do not adequately address the needs of different conceptual perspectives on business processes. For example, metrics may be undefined for one or more of the following: components of a business process, business process perspectives, critical information for auditing the business process, and business process presentation layers. This paper addresses these gaps by paying attention to PBF and various conceptual perspectives for a successful PBF detection approach."
c,125,2018,"Although Neural Networks (NN) are extremely useful for the solution of several problems such as object recognition and semantic segmentation, the NN libraries usually target devices which face several drawbacks such as memory bottlenecks and limited efficiency (e.g. GPUs, multi-core processors). Fortunately, the recent implementation of Hardware Neural Networks aims to tackle down this problem and for that reason several researchers had turn back their attention to them. This paper presents the Systematic Literature Review (SLR) of the most relevant HNN works presented in the last few years. The main sources chosen for the SLR were the IEEE Computer Society Digital Library and the SCOPUS indexing system, from which 61 papers were reviewed according to the inclusion and exclusion criteria, and of which after a detail assessment, only 20 papers remained. Moreover, the increase in the number of papers per year reflects that the interest in HNN had been growing up. Finally, the results show that the most popular NN hardware platforms are the FPGAs-based."
d,126,2018,"the development of localization techniques has grown rapidly, especially localization techniques that use satellites known as the Global Positioning System. Localization techniques using this satellite cannot work in buildings; therefore research for localization techniques in buildings has been the focus of researchers in the last decade. Localization techniques in the area that cannot be covered by satellite can be split into two groups namely techniques signal using Bluetooth Low Energy (BLE) and techniques using the access point or Wi-Fi. Most researcher still face problems in localization technique in building known as Indoor Position System (IPS) implementation, especially to escalation the precision of techniques. In this paper, we presented a assessment of the papers (2006-2016) on indoor position system that focus on accuracy and placement of sensor for both BLE and Wi-Fi. This paper is a Systematic Literature Review (SLR) establishes a fusion exploration based on the exploration model by PRISMA checklist. The finishing identifies of chosen major papers for the initially phase had 25 major papers; the complete texts of 25 major papers were explored. Then, we implemented the synthesis to construct parameter and aspects that affect accuracy of indoor position system implementation for BLE and Wi-Fi. Base on review, we can conclude that the fingerprint of sensor play important role to increase accuracy of system."
a,127,2017,"The main purpose of this study is to determine the Information System (IS) adoption model that applied to Enterprise 2.0 applications. To achieve the objectives, we conduct the systematic literature review method that summaries all the studies required in IS adoption model. We have discovered 257 papers, and selected into 15 best papers through several stages using Kitchenham method. During systematic literature review, the research questions can be identified, analyzed and answered. A result is a variety approach depends on several factors such as organization, environment, technology, personal, competence and others."
b,128,2020,"Intelligent tutoring systems (ITSs) are computer programs that provide instruction adapted to the needs of individual students. Dialog systems are computer programs that communicate with human users by using natural language. This paper presents a systematic literature review to address ITSs that incorporate dialog systems and have been implemented in the last twenty years. The review found 33 ITSs and focused on answering the following five research questions. a) What ITSs with natural language dialogue have been developed? b) What is the main purpose of the tutoring dialogue in each system? c) What are the pedagogical features of the teaching process performed by the ITSs with natural language dialogue? d) What natural language understanding approach does each system employ to understand students â¢ utterances? e) What evidence exists related to the evaluation of ITSs with natural language dialogue? The results of this review reveal that most ITSs are directed toward science, technology, engineering, and mathematics (STEM) domains at the university level, and the majority of the selected ITSs implement the expectations and misconceptions tailored approach. Furthermore, most ITSs use dialog to help students learn how to solve a problem by applying rules, laws, etc. (the apply level in Bloom â¢s taxonomy). With regard to the instructional approach, the selected ITSs help students write correct explanations or answers for deep questions; assist students in problem solving; or support a reflective dialogue motivated by either previously provided content or the result of a simulation. Additionally, we found empirical evaluations for 90.91% of the selected ITSs that measure the learning gains and/or assess the impacts of different tutoring strategies."
c,129,2020,"To analyze the state of the art of machine learning-based disease profiling and personalized treatments, we review the relevant literature included in top peer-reviewed journals and evaluate the coverage according to the ICD-11 framework. We identify advantages, but also research needs and limitations within the ICD-11 disease categories to foster the adaptation of these new E-health technologies."
d,130,2020,"From a perspective of behavioral change, we review medical chatbot literature included in top peer-reviewed journals and conferences, and build up a comprehensive picture. We examine literature on how people feel about using a medical chatbot and based on that how far chatbots are useful to change harmful behavior. To structure the review we use the theory of planned behavior and the theory of reasoned action. Based on this we conclude five design-oriented recommendations. We expect this research to identify behavioral aspects that contribute to the acceptance, usage and effectiveness of medical chatbots in future."
a,131,2020,"We review literature in top journals and conferences on the usage of deep learning for medical image analysis in modern healthcare. As a result it is shown that deep learning offers unique capabilities and breakthroughs in identifying, classifying and segmenting different kinds of medical images, especially related to cancer in the breast, lung, and brain."
b,132,2018,"Open Government Data (OGD) has been growing as a field of research in digital government. Using a systematic review method, this paper sought to capture the status of research in OGD over the last five years from 2012-2018. The key findings indicate that OGD is an emergent research area with no existing theoretical frameworks. The absence of design science methods suggests an opportunity to create digital government platforms based on OGD best practices. There are also strong legal concerns concerning the opening up of government data, the format of the data, and the privacy / security implications of such exposure. There is limited research on OGD for business benefit nor developing countries. We highlight some gaps for further research and identify potential opportunities."
c,133,2020,"From a strong practical point of view, we offer an overview of virtual and augmented reality solutions in medicine. We thus analyzed practical and industrial work included in peer-reviewed articles and conference proceedings."
d,134,2020,"Nowadays, most software systems manage large amount of data. The clients depend heavily on these data and expect them to be available at all times. To use and manage these data in an efficient way and to ensure availability, the data replication technique is applied. So far, three basic models for replication exist with their variants. This paper reviews these three basic models of replication techniques and their variants with regard to how the load is distributed among replicas, what the total throughput is for these set of replicas, and which type of consistency models is supported by them."
a,135,2017,Requirements volatility is a crucial risk factor in software projects as it directly results in cost and time overruns. Accurately predicting requirements volatility is important for better project management. This paper presents a systematic literature review that focuses on the prediction of the requirements volatility. This literature review aims to answer four research questions: 1) how is requirements volatility prediction applied to different software development methods? 2) What are the machine learning algorithms used to predict requirements volatility in software development? 3) What are the attributes (predictors) used to predict requirements volatility in software development? 4) What are the performance metrics for evaluating existing prediction models? This study presents predictors used in the literature and their performances.
b,136,2020,"Due to the growing demand for efficient and precise surgical options, there has been a push in the field of development of medical robot systems in recent years. This paper provides an overview of the current status and development of medical robots in surgery through a systematic literature review of research. The classification is made into minimally invasive and non-invasive robotic assistance. An introduction and definition of medical robots is provided. The advantages and disadvantages of the applications are highlighted. A summary and insight into future work is presented."
c,137,2019,"The distinct abilities of older adults to interact with touchscreen devices have motivated a wide range of contributions in the form of design guidelines, which aim at informing the design for the aging population. However, despite the growing effort by the research community, many challenges still remain in translating these research findings into actionable design guidelines, with reports hinting scant adoption or implementation issues, which ultimately hurt the development of more accessible interactive systems. In this systematic literature review, we look at the research-derived design guidelines that set the foundation for design guideline compilations and standards, analyzing the aforementioned issues from the perspective of experts trying to discover, classify, and evaluate the work on the area of touchscreen design guidelines for older adults. The review analyses 52 research articles resulting in 434 research-derived design guidelines for touchscreen applications. These guidelines are analyzed using a taxonomy that considered the older adults ability evolution and the design aspects that are the target of the recommendations. The results point to the use of different definition of older adults, which go as early as 55+, with the design of displays and interaction styles to accommodate to vision and dexterity declines as the most prominent areas of research. However, proposed guidelines and recommendations were validated in only 15% of articles analyzed. The analysis also revealed that identifying guidelines and characterizing their focus in terms of ability declines and design aspects addressed is a demanding activity and prone to error, given the quality of reporting and details offered in research articles."
d,138,2020,"In this article, with the increased disruptions faced by businesses and the occurrence of natural disasters in the world, supply chain resilience remains a major challenge especially for small- and medium-sized enterprises (SMEs). Despite the relevance of SMEs to the economy, there is limited scholarly work on resilience practices in SMEs and a limited understanding of how SMEs can achieve resilience. To understand the role of supply chain resilience in SMEs, we undertake a systematic literature review (SLR), which results in the identification and analysis of 101 journal articles, published between 2006 and 2019, on SME supply chain resilience. Our analysis into SME supply chain resilience highlights four focal areas: 1) the role of collaboration and culture; 2) the role of SMEs â¢ capabilities; 3) the role of Information Systems; and (4) the role of cost and financing. Our SLR investigation identifies future research directions and focal areas tailored to SMEs to help them to assess and develop their supply chain resilience."
a,139,2019,"Regression testing is the essential process of software maintenance and evolution phase of the software development life cycle for assuring the quality and reliability of updated software. Test case prioritization is the technique of regression testing to reduce the time and effort required for regression testing. Search-based algorithms are used to enhance the efficiency and effectiveness of the method. Among these search-based optimization algorithms, genetic algorithms are becoming more popular among researchers since the last decade. In this paper, we are doing a systematic literature review, i.e., a secondary study of test case prioritization using genetic algorithms. The objective of this review is to examine and classify the current state of use of the genetic algorithm in test case prioritization. In other words, to give a base for the advancement of test case prioritization research using genetic algorithms. With the use of the systematic literature review protocol, we selected the most relevant studies (20 out of 384) from the appropriate repositories by using a set of search keywords, inclusion/exclusion criteria and the quality assessment of studies. The data extraction and synthesis process and the taxonomic classification are used to answer the research questions. We also performed a rigorous analysis of the techniques by comparing them on research methodology, the prioritization method, dataset specification, test suite size, types of genetic algorithms used, performance metrics, and the validation criteria. The whole process took four months for comprehensive analysis and classification of primary studies. We observed that the parameter settings, the type of operators, the probabilistic rate of operators, and fitness function design have a significant impact on the quality of the solutions obtained. This systematic literature review yields that genetic algorithms have great potential in solving test case prioritization problems, and the area is open for further improvements. Future researchers can fill the research gaps by following the suggestions given in the review. From this review, we found that the use of the appropriate approach can make a genetic algorithm based test case prioritization one of the effective methods in regression testing."
b,140,2018,"Quality management and the practices associated with quality improvement continue to shape global business. One such practice is reducing the Cost of Quality (CoQ) in companies; yet both research and industry indicate that CoQ as a competitive tool for continuous improvement is not yet fully adopted by firms. The objective of this study is to assess the factors that prevent the successful implementation of CoQ. The study adopted systematic literature review as the research methodology and the review was conducted on papers published over an eleven year period in the Emerald Insight database between 2007 and 2018. The findings suggest that multiple factors affect the implementation of CoQ programmes. Twenty key factors were identified, with measurement and improvement, return on investment, management support, awareness, and strategic alignment as the most listed contributors to poor CoQ implementation. The paper concludes with key findings and recommendation."
c,141,2017,"Agile software development, become one of the most applied methods and principles in software development industry. That methodology offers some advantages in managing time and requirements from the system user. However, it has some disadvantages in managing project resource such as codes, documentation, and knowledge which emerge in the developments process. Other weakness of agile software development was too focused on the functionality of the system. Those problems can be solved by combining it with other methods. This paper is a systematic literature review which used PRISMA as a method to gather literature regarding the improved agile methodologies from three paper databases, including Science Direct, IEEE Xplore, and ProQuest. As the result, we obtained 15 papers which explained the improved agile software development in all of its aspects, such as its time, documentation and usability."
d,142,2019,"In healthcare, mobile-based interventions support the improvement of clinical process and result in a positive behavioral change and improve the patients' health condition. This study aims at reviewing mobile applications documented for pain management in the scientific databases, to identify the key factors that are vital for pain management. In this research, a systematic literature review was conducted on the selected studies collected from five scientific databases: Medline, PubMed, EMBASE, Web of Science and Scopus. After applying the inclusion and exclusion criteria and performing the quality assessment, twenty-five studies were finalized. It has been observed that the apps were not all-inclusive in features to provide an effective pain self-management solution. As found from the review, the general features of the pain management mobile applications are pain information, pain coping strategy, social support, sub-goals and achievements, self-reporting, feedback, and patient report. Some apps involved psychological interventions. A prominent technique found was cognitive behavior therapy. This study has contributed to the body of knowledge by proposing a conceptual model in guiding the development of pain management mobile applications. The conceptual model was evaluated by a panel of experts to evaluate comprehensiveness, accuracy, and dependencies among the elements of the model, and the appropriateness of the proposed model. Experts recognized the importance of pain management and provided positive feedback to the proposed model."
a,143,2018,"Self-adaptive Systems (SaSs) operate under uncertainty conditions and have intrinsic properties that make their modeling a non-trivial activity. This complexity can be minimized by using Domain-Specific Modeling Languages (DSMLs), which may be created by extending Unified Modeling Language (UML). In face of this, we propose investigating how the UML has been customized to create DSMLs that provide proper support for SaSs modeling. To achieve this, we performed a Systematic Literature Review (SRL) by retrieving studies with snowballing technique, selecting studies according to inclusion and exclusion criteria, and extracting and analyzing data to answer our research questions. As the outcome, we retrieved 786 studies and selected 16 primary studies published between 2005 and 2017. The results reveal that the class diagram has been customized through the profile-based mechanism to provide proper support to analysis and design of context-awareness and self-adaptiveness properties."
b,144,2019,"Nowadays, learning technologies have solved problems of distance, time, and cost in learning. However, another problem with e-learning, which is lack of student motivation, still remains. Some researchers must apply gamification in e-learning with the aim of giving participation and increasing student motivation. Characteristics of different users, the same learning content, and static gamification elements do not increase the expected motivation. To overcome this problem, gamification must be adapted to the characteristics of the learners. This study uses a Systematic Literature Review (SLR) to analyze research components, methods, and framework used in adaptive gamification. The first step is to determine the research question (RQ) and then to search on several kinds of literature published in popular journal databases, namely IEEE Xplore, Science Direct, Scopus, Springerlink and ACM from 2011 - 2019. As a result of a review of 25 selected articles, there are four components of adaptive gamification, namely, player/learner profiles, learning style, behavior, and skill/knowledge. Meanwhile, there are eleven types of methods used in adaptive gamification. The most popular method is scoring and Felder-Silverman Learning Style Model (FSLSM). Generally, the proposed framework consists of three elements namely adaptive gamification engine, adaptive component, and gamification display."
c,145,2019,"Organizations are needed of flexible structures to achieve their aims. In the software arena, Self-organization is the one of the principles of the agile methodologies as defined in the agile manifesto. Autonomous agile teams are a way to conform self-organization. The main objective of this paper is to know about the how autonomous agile teams organize themselves and what are the benefits and the challenges faced by these teams. A systematic literature review was conducted to find answers to these questions. 23 relevant papers were identified as primary sources to define and study autonomous and self-organized agile team. From the review, it was found that there are many benefits and challenges of autonomous agile teams and the various strategies to overcome the challenges."
d,146,2018,"Bad smells are sub-optimal code structures that may represent problems that need attention. We conduct an extensive literature review on a huge body of knowledge from 1990 to 2017. We show that some smells are much more studied in the literature than others and some of them are intrinsically inter-related (which). We give a perspective on how the research has been driven across time (when). In particular, while the interest in duplicated code emerged before the reference publications by Fowler and Beck and by Brown et al., other types of bad smells started to be studied only after these seminal publications, with an increasing trend in the last decade. We analyzed aims, findings, and respective experimental settings and observed that the variability on these elements may be responsible for some apparently contradictory findings on bad smells (what). Moreover, while in general bad smells of different types are studied together, only a small percentage of these studies actually investigate the relations between them (co-studies). In addition, only few relations between some types of bad smells were investigated, while there are other possible relations for further investigation. We also noted that authors have different levels of interest in the subject, some of them publishing sporadically and others continuously (who). We observed that scientific connections are ruled by a large  Å small world connected graph among researchers and several small disconnected graphs. We also found that the communities studying duplicated code and other types of bad smells are largely separated. Finally, we observed that some venues are more likely to disseminate knowledge on Duplicate Code (which often is listed as a conference topic on its own), while others have a more balanced distribution among other smells (where). Finally, we provide a discussion on future directions for bad smell research."
a,147,2018,"The aim of Ambient Assisted Living (AAL) is to help people (e.g., elderly, children) to have an independent and monitored life with the use and assistance of technology. For this population, specific technologies have been developed; many of those technologies (i.e., hardware, software) use sensors and embedded systems. Although there are many studies that address AAL approaches, no systematic literature reviews have been presented that cover hardware and software working together and take into account specific disabilities. Therefore, in this paper a systematic literature review of primary studies related to AAL is presented, which is focused on establishing software and hardware relations working together in order to solve or support specific disabilities. In this study, 3297 articles were extracted, from them only 48 were selected after applying the corresponding inclusion and exclusion criteria."
b,148,2019,"Admissibility of Evidence is the eligibility of particular pieces of evidence for inception as part of the evidence in a case. Admissibility means the character or quality to be accepted and allowed to be presented or introduced as evidence in court. To be admissible means capable of being legally admitted or allowable or permissible as evidence or worthy of gaining entry or being admitted. This study focus on carrying out a systematic literature review and meta-analysis on Digital Evidence Admissibility. The methodology employed in this study was the querying of four academic database resources systematically and fundamentally identifying kinds of literature related to digital evidence admissibility through identification, screening, eligibility and inclusion criteria. The advantage of this study is revealing the gap and trends in digital evidence admissibility as the article published between the period of 2015 through 2018 are relative with the following variation 64%, 21% 7%, for the following respectively, IEEE, Science Direct, ACM Digital library, as well as Research Gate. While at the period under review, 2019 is yet to record publication in the field of research in Digital Evidence Admissibility. The period under review witnessed a low academic publication in the field of Digital Evidence admissibility. This research will aid in projecting future research in the aforementioned research field."
c,149,2018,"Digital transformation is the integration of digital technology into all sectors of a business, fundamentally altering how you perform and bring value to customers. It has an impact and benefits on the business models, the operational processes and the customer experience. Moreover, when the digital transformation has extended to the entire areas of business activities, some of sectors has a chance to developed with more scenarios in the future comparing from others. The main goal of this paper is to give a general review of a systematic literature on digital transformation from a previous study to the latest five years. The results indicated that organizations should modify their business plan or policy to a new digital business model in order to achieve their goals. This mostly shows in the applying of operation and process management. In this study, it is very difficult to identified the entire opportunities and challenges of digital transformation, but also happened these problems in the previous study."
d,150,2020,"EA (Enterprise Architecture) visualization methodologies have been explored by researchers and engineers to conduct EA modeling. The objectives of EA modeling are to clarify enterprise strategies, visualize business processes, and model information systems to manage resources, improve organization structure, adjust information strategy, and create new business value. Therefore, EA models can be broadly applied in various fields. For example, the applications include business modeling, information system architecture design, technology infrastructure configuration, software maintenance, and system security analysis. As the primary source of information, EA models are of paramount importance to researchers, architects, and developers. However, up to now, the purpose and means of these EA visualization methods have never been systematically analyzed and discussed, and a generalized EA visualization methodology with the ability to meet different demands is needed. The paper narrows this gap by conducting a systematic literature review on enterprise architecture visualization methodologies. In this study, 112 papers are retrieved by a manual search in 5 academic databases, a systematic literature review on EA visualization is explained to show a systematized category of visualization approaches, and then a general visualization approach is proposed by systematically reviewing the papers. Finally, the paper is concluded by discussing the contributions and limitations of the study."
a,151,2018,"In an organization knowledge sharing is needed in an effort to innovate. Knowledge sharing process required an enabler that will help the process. We know that today knowledge is one of the main factors in generating innovation, so this research aims to provide information, description and more comprehensively identifies the enablers of knowledge sharing and the relationship between knowledge sharing and innovation by using empirical literature review. Some steps need to be done to conduct a systematic and comprehensive literature review. In this research, the stages are Identification of potential study, Study Selection based on criteria, Quality assessment, Data Extraction, and Data analysis. Based on the literature review systematically compiled it is obtained 15 main articles that correspond to the purpose of research. From these 15 articles can be identified various enablers that are important in knowledge sharing, the type of innovation, as well as the relationship between knowledge sharing and innovation."
b,152,2018,"Data are substantial in the process of decision making. One of the important steps to provide data is data collection. Currently, there are several methods of data collection system that have been used. Each method has features and technology characteristics that can differ from one another. The study conducts a systematic literature review (SLR) with the objective to understand the feature, technology, method for data collection system in the previous research. This understanding can be used as a base for developing a data collection system platform. The first step is determining the research question (RQ), then searching the previous research on several database journals e.g. IEEE Xplore, Scopus, SpringerLink, ScienceDirect dan ACM. After reviewing 23 chosen articles, the study conclude that there are several features in data collection system that can be divided into categories of preparation, collection, transmission and security; Cloud, portable devices (mobile phone) and GPS are some of the technologies that are widely used to support data collection system; PAPI, CATI, CAPI, and CAWI currently exist method for data collection system; CAPI is the most widely used choice in previous research; The integration of several technologies or methods increase the functionality on the implementation."
c,153,2019,"Learning can be performed in various ways and can utilize different technologies. This paper presents a review of current and previous research to understand the use of virtual reality technology for learning. This paper used a systematic literature review (SLR) as a method. Research question (RQ) was determined in the first step. The query for searching the previous research on popular database journals was generated from previously created RQ. Popular journals included IEEE Xplore, ScienceDirect, SpringerLink, Scopus, and ACM Digital Library. Thirty-two related articles were produced from the search, then reviewed. The study concluded that there were four purposes of using virtual reality for learning, two types of devices used, and two types of user experiences."
d,154,2018,"Currently, software organizations are implementing agile methodologies in global software development (GSD) because of low development cost, schedule and high quality product. However, GSD project is complex undertaken because of it distributed dimensions especially when the agile methodologies are concerned. The objective of this study is to identify the human related factors that can negatively influence agile practices in GSD organizations, and proposed a hypothetical model of the identified challenges related to the scaling agile methodologies. A Systematic Literature Review (SLR) method was used to identify the challenges. In the findings, a total of eleven challenges were identified using SLR. This study also reported the Critical Challenges (CChs) for scaling agile methodologies using a criterion of the challenges having a frequency Ã¢â °Â¥ 50 %. Findings reported the six out of eleven challenges as critical challenges in scaling agile methods. Based on the identified challenges, a hypothetical model was presented that is highlighted a relationship between identified challenges and the implementation of agile methodologies in GSD environment."
a,155,2017,"Human behavior inside organizations is considered the main threat to organizations. Moreover, in information security the human element consider the most of weakest link in general. Therefore it is crucial to create an information security culture to protect the organization's assets from inside and to influence employees' security behavior. This paper focuses on identifying the definitions and frameworks for establishing and maintaining information security culture inside organizations. It presents work have been done to conduct a systematic literature review of papers published on information security culture from 2003 to 2016. The review identified 68 papers that focus on this area, 18 of which propose an information security culture framework. An analysis of these papers indicate there is a positive relationship between levels of knowledge and how employees behave. The level of knowledge significantly affects information security behavior and should be considered as a critical factor in the effectiveness of information security culture and in any further work that is carried out on information security culture. Therefore, there is a need for more studies to identity the security knowledge that needs to be incorporated into organizations and to find instances of best practice for building an information security culture within organizations."
b,156,2018,"Background: Requirement prioritization plays key role in software development process. It is essential to prioritize the requirements for making the correct decision for either a single or multiple release of a product. In this paper we performed a systematic analysis on some of the significant factors like importance of requirements, risks, cost and time in context of requirement prioritization. Objective: With benefits these prioritization methods also have some limitations and shortcoming that are brought up in this paper. Stakeholders, managers, developers or their representatives make decisions for prioritization of requirements. Many techniques are analyzed how to manage these prioritizations considering general goals and limitations. Method: For the identification and analyzing of research articles published during 2009-2017, Systematic Literature Review based method is used in this paper. Results: In recent researches 40 different requirement prioritizations techniques has been used. This SLR also shows the major research gaps regarding techniques and tools for software requirements prioritization. Conclusion: This research shows the major prioritization techniques and tools for requirements elicitation. Tools & techniques identified in this study will assist future researchers to expend their views in the context. Moreover, it will help requirement engineers and practitioners to choose requirement prioritization techniques and tools according to their needs."
c,157,2018,"Numerous problems investigated in Software Aging and Rejuvenation (SAR) research are also of interest of other research communities. One of these problems is memory leak detection, which is one of the most recurrent topic in the SAR literature in its twenty-three years. Due to this major interest of SAR researchers on memory leak detection techniques, naturally a question emerges: how other areas deal with this problem? To answer this question we surveyed the main scientific digital libraries, from 1982 to 2017, looking for studies on memory-leak detection that were not originated in the SAR community. We found 105 papers that matched our inclusion and exclusion criteria. Our method was based on the systematic mapping review approach. The empirical findings revealed several intersections of SAR research with studies conducted in other areas, offering interesting insights on different research cooperation opportunities."
d,158,2019,"The elderly experience difficulties when using design-for-all user interfaces. Moreover, their age-related issues increase over time. In the digital age, the mobile phone is most popularly used by the elderly to support their daily living and help them live more independently. However, mobile user interface guidelines customized for specific elderly groups are lacking. Cognitive load is one of the crucial factors that software developers and designers should consider when developing user interfaces for the elderly. To understand these aspects in more detail, we need to review the current literature and identify existing research gaps. This study investigates what are the focuses of mobile user interface design for the elderly by using a systematic mapping review method. Currently, this research presents a generic view, classification type, and new cognition aspects of overall user interface designs for the elderly. The findings show that this topic deserves more attention than it has received."
a,159,2019,"As the digital world is growing widespread crime in the cyberspace is also increasing. Knowledge sharing and utilization of services attracts to use the digital devices, but the concern here is malicious usage of the system. If the crime takes place over the network how to collect it, analyze it and investigate based on the evidences. So, the role of forensic and incident response is crucial here. Digital forensics is categorized in Disk, Live, Network and Mobile Forensics. Anomaly or attack over the network comes under network forensic branch. In this paper extensive literature review is performed to compare the latest intrusion detection systems and based on the learning's a system is proposed which covers the peer to peer architecture system and utilization of web robots to trace the attack and log it in a form which will be a useful input for forensic investigation and analysis work."
b,160,2020,"Big data (BD) analytics is one of the critical components in the digitalization of the oil and gas (O&G) industry. Its focus is managing and processing a high volume of data to improve operational efficiency, enhance decision making and mitigate risks in the workplace. Enhanced processing of seismic data also provides the industry with a better understanding of BD applications. However, the industry still exercises caution in adopting new technologies. The slow pace of technology adoption can be attributed to various causes, from the obstacles to the integration with existing systems, to cybersecurity for defending the BD system against cyber attacks. In some applications using wearable devices, physiological and location-tracking data also causes concerns related to workplace privacy implications. These shortcomings give rise to uncertainties about the practical benefits and effectiveness of applying BD in O&G activities. The objective of this paper is to perform a systematic review of BD analytics within the context of the O&G industry. This paper attempts to evaluate technical and nontechnical factors affecting the adoption of BD technologies. The study includes BD development platforms, network architecture, data privacy implications, cybersecurity, and the opportunities and challenges of adopting BD technologies in the O&G industry."
c,161,2017,"In service oriented computing, Web service selection is an important part of Web service composition. The Web service composition is achieved by solving the Web service concretization problem. The literature presents two types of Web service concretization approaches: local optimization approaches and global optimization approaches. There are three types of algorithmic methods in the global optimization approaches: optimal methods, sub-optimal methods, and soft constraints-based methods. The bio-inspired algorithms are sub-optimal methods. This paper will first present a hierarchical taxonomy of web service concretization approaches. Then we conduct a systematic review on the current research of Web service concretization based on three bio-inspired algorithms, namely, ant colony optimization algorithms, genetic algorithms, and particle swarm optimization algorithms. Based on the findings from the systematic review, this paper also discusses the underlying applications of bio-inspired algorithms to the data-intensive service concretization problems."
d,162,2019,"The aim of this paper is to provide a systematic literature review of blockchain hardware acceleration. Blockchain technology has achieved significant attention in recent years particularly in the area of cryptocurrency however it is gaining popularity in other applications such as supply chain management and e-government. Based on a structured, systematic review of the relevant literature, we present a classification of the primary areas in blockchain technology that make use of heterogeneous hardware for accelerating certain blockchain functions. Based on these findings, we identify various research gaps and future exploratory directions that are anticipated to be of significant value both for academics and industry practitioners."
a,163,2020,"Blockchain, a form of distributed ledger technology has attracted the interests of stakeholders across several sectors including healthcare. Its' potential in the multi-stakeholder operated sector like health has been responsible for several investments, studies, and implementations. Electronic Health Records (EHR) systems traditionally used for the exchange of health information amongst healthcare stakeholders have been criticised for centralising power, failures and attack-points with exchange data custodians. EHRs have struggled in the face of multi-stakeholder and system requirements while adhering to security, privacy, ethical and other regulatory constraints. Blockchain is promising amongst others to address the many EHR challenges, primarily trustless and secure exchange of health information amongst stakeholders. Many blockchain-in-healthcare frameworks have been proposed; some prototyped and/or implemented. This study leveraged the PRISMA framework to systematically search and evaluate the different models proposed; prototyped and/or implemented. The bibliometric and functional distribution of all 143 articles from this study were presented. This study evaluated 61 articles that discussed either prototypes or pilot or implementations. The technical and architectural analysis of these 61 articles for privacy, security, cost, and performance were detailed. Blockchain was found to solve the trust, security and privacy constraints of traditional EHRs often at significant performance, storage and cost trade-offs."
b,164,2018,"Because of the dynamic environments of business and IT, achieving any alignment between the two fields has become challenging. In view of its multiple viewpoints and artifacts, the discipline of enterprise architecture (EA) is often regarded as an effective methodology to deal with business-IT alignment (BITA) issues, and thus has attracted plenty of research. This article conducts a systematic literature review of BITA research using EA. Six questions are answered through 5W1H (When, Who, What, Why, Where, How) analysis; these questions aim to acquire a thorough understanding of BITA from the perspective of EA, to discover weak points in the status quo, and to identify future research directions."
c,165,2018,"Compressive Sensing (CS) is a new sensing modality, which compresses the signal being acquired at the time of sensing. Signals can have sparse or compressible representation either in original domain or in some transform domain. Relying on the sparsity of the signals, CS allows us to sample the signal at a rate much below the Nyquist sampling rate. Also, the varied reconstruction algorithms of CS can faithfully reconstruct the original signal back from fewer compressive measurements. This fact has stimulated research interest toward the use of CS in several fields, such as magnetic resonance imaging, high-speed video acquisition, and ultrawideband communication. This paper reviews the basic theoretical concepts underlying CS. To bridge the gap between theory and practicality of CS, different CS acquisition strategies and reconstruction approaches are elaborated systematically in this paper. The major application areas where CS is currently being used are reviewed here. This paper also highlights some of the challenges and research directions in this field."
d,166,2018,"This paper reports on results from a systematic review that characterizes the state-of-the-art on cost reduction for mutation testing. It analyzes the evolution of research on this topic and its underlying goals and techniques, and identifies metrics used to measure cost reduction. The mixed search strategy used automatic search, snowballing, and a survey of authors of primary studies. The analysis is based on a set of 165 peer-reviewed studies, from which 146 present either original or updated approaches and results for cost reduction of mutation testing. A list of 6 main goals for cost reduction is presented, and 22 techniques were identified. Historically, 18 metrics have been used to measure the gains and losses observed in experimental studies. In the last decade, substantial growth in the number of published studies was observed, particularly among techniques such as selective mutation, evolutionary algorithms, control-flow analysis, and higher order mutation."
a,167,2017,"We systematically reviewed 64 user-study papers on data glyphs to help researchers and practitioners gain an informed understanding of tradeoffs in the glyph design space. The glyphs we consider are individual representations of multi-dimensional data points, often meant to be shown in small-multiple settings. Over the past 60 years many different glyph designs were proposed and many of these designs have been subjected to perceptual or comparative evaluations. Yet, a systematic overview of the types of glyphs and design variations tested, the tasks under which they were analyzed, or even the study goals and results does not yet exist. In this paper we provide such an overview by systematically sampling and tabulating the literature on data glyph studies, listing their designs, questions, data, and tasks. In addition we present a concise overview of the types of glyphs and their design characteristics analyzed by researchers in the past, and a synthesis of the study results. Based on our meta analysis of all results we further contribute a set of design implications and a discussion on open research directions."
b,168,2020,"Cloud systems, as any other system, must be reliable. This means that the system should respond correctly in presence of failures, which are quite probable in a distributed, largely independent, system as cloud systems are. Thus, it is important that cloud systems become fault tolerant, ensuring safe recovery from failures. Since failures in clouds may come from several different sources, although a major role comes from communication failures, the techniques that can be applied to assure reliability are also very different. This survey presents a systematic review of solutions to provide fault tolerance in open source clouds. Our goal with this review is to provide to cloud managers a guided approach to choose a solution for a given problem or system."
c,169,2019,"Background: Feature selection techniques are important factors for improving machine learning models because they increase prediction accuracy and decrease the time to create a model. Recently, feature selection techniques have been employed on software quality prediction problems with different results and no clear indication of which techniques are frequently used.Objective: This study aims to conduct a systematic review of the application of feature selection techniques in software quality prediction and answers eight research questions.Method: The review evaluates 15 papers in 9 journals and 6 conference proceedings from 2007 to 2017 using the standard systematic literature review method.Results: The results obtained from this study reveal that the filter feature selection method was the most commonly used in the studies (60%) and RELIEF was the most employed among this method, and a limited number of studies employed an ensemble method. Several studies used public datasets available in the PROMISE software project repository (60%). Most studies focused on software defect prediction (classification problem) using area under curve (AUC) as a primary evaluation measure, whereas only two studies focused on software maintainability prediction (regression problem) using mean magnitude of relative error (MMRE) as a primary evaluation measure. All selected studies performed k-fold cross-validation to evaluate model accuracy. Individual prediction models were mostly employed and ensemble models appeared only in three studies. Naive Bayes was the most investigated among individual models, whereas Random forest was the most investigated among ensemble models.Conclusion: Feature selection techniques used by selected primary studies have a positive impact on the performance of the prediction models. Further, both ensemble feature selection method and ensemble models have the ability for increasing prediction accuracy over single methods or individual models and have reported improvement in the prediction accuracy; however, the application of these techniques in software quality prediction is still limited."
d,170,2018,"Today, mobile devices are being widely used in personal and professional life. By increasing the popularity of touchscreen platform as an input method in mobiles phones, touch gesture behaviour is becoming more significantly important in interaction with the phone. Due to increasing demand for safer access in touchscreen mobile phones, old strategies like pins, tokens, or passwords have failed to stay abreast of the challenges. By utilizing touch gesture behaviour biometric techniques, the authentication mechanism will improve and would make it more difficult for a shoulder surfer to replay the password, even if he observes the entire gesture. The purpose of this research is to conduct a systematic literature review SLR for the current state-of-the-art in the field of touch features gesture. The results of this research identified thirteen touch features followed by the previous studies (PSs) to authenticate users. However, based on the schemes followed by the previous studies (PSs) there was not any scheme that extract and use all touch features for mobile user authentication. There are number of areas of future work that could be carried out to advance upon this research and within the area of authentication on mobile devices. Each of the pervious study (PS) extracted only one or more features. The maximum number of features used in S14 year 2014 and S15 year 2015 were seven features. Extracting thirteen touch features and use them for the authentication will be one contribution in future work."
a,171,2017,"Since their origin in 1990's, recommender systems have changed the intelligence of both human and web. Many research articles have been published in various domains of recommender systems such as collaborative, content, context, hybrid, location, social, tag, group and others. We include the reputed papers that highlight, analyze and synthesized study only on the group recommender system (GRS) domain. We focused on evaluation techniques."
b,172,2020,"Recently, interest in the effects of radio frequency (RF) on biological systems has increased and is partially due to the advancements and increased implementations of RF into technology. As research in this area has progressed, the reliability and reproducibility of the experiments has not crossed multidisciplinary boundaries. Therefore, as researchers, it is imperative to understand the various exposure systems available as well as the aspects, both electromagnetic and biological, needed to produce a sound exposure experiment. This systematic review examines common RF exposure methods for both in vitro and in vivo studies. For in vitro studies, possible biological limitations are emphasized. The validity of the examined methods, for both in vitro and in vivo, are analyzed by considering the advantages and disadvantages of each. This review offers guidance for researchers to assist in the development of an RF exposure experiment that crosses current multidisciplinary boundaries."
c,173,2019,"Video surveillance systems obtain a great interest as application-oriented studies that have been growing rapidly in the past decade. The most recent studies attempt to integrate computer vision, image processing, and artificial intelligence capabilities into video surveillance applications. Although there are so many achievements in the acquisition of datasets, methods, and frameworks published, there are not many papers that can provide a comprehensive picture of the current state of video surveillance system research. This paper provides a comprehensive and systematic review on the literature from various video surveillance system studies published from 2010 through 2019. Within a selected study extraction process, 220 journal-based publications were identified and analyzed to illustrate the research trends, datasets, methods, and frameworks used in the field of video surveillance, to provide an in-depth explanation about research trends that many topics raised by researchers as a focus in their researches, to provide references on public datasets that are often used by researchers as a comparison and a means of developing a test method, and to give accounts on the improvement and integration of network infrastructure design to meet the demand for multimedia data. In the end of this paper, several opportunities and challenges related to researches in the video surveillance system are mentioned."
d,174,2017,"Recent developments in Learning Analytics have attracted much attention from researchers and practitioners, as well as diverse stakeholders, in exploring the potential for Learning Analytics to enhance learning and teaching practices. Hence, this paper presents a literature review of the use of the Learning Analytics Intervention implemented in various educational institutions in order to improve students' success and engagement in their learning, particularly for at-risk students. The aim of this paper is to provide an overview of how Learning Analytics Intervention has been utilised in the educational institution, the types of theory applied in the intervention and what tools are used in the intervention. Additionally, the application of Learning Analytics Intervention, its purpose, effects and examples are also discussed in this paper. The findings show that most of the Learning Analytics Interventions have an impact on students' success in their learning, although some of them have little impact or no significant effect, and various Learning Analytics tools and strategies are discovered. Lastly, it is suggested that further Learning Analytics Interventions should be developed in order to meet the needs of students."
a,175,2017,"In this review paper, the studies dedicated to identifying the learning of Object Oriented Programming through Serious Games(SGs) and applying different programming approached applied were investigated. This systematic review attempts to gather all evidence that helps answer three specific research questions: identification of SGs developed or incorporated for learning OOP, OOP concepts covered in those games, and programming approaches applied. Three different ways were found as result of the learning through SGs: learning by playing games, learning by creating games and learning by using game related tools. Majority of studies covered all basic OOP concepts. The programming approaches applied includes object first, concept first, GUI first and code first programming approaches. Result indicates learning by playing games is most frequently used and effective strategy followed by learning by using game related tools. The most common programming approach applied is Game as first approach."
b,176,2018,"It has been frequent the discussion about the teaching and learning of Programming, from the initial series to the undergraduate courses. It is noticed that many students have difficulty to learn programming by several reasons: methodology, tools, programming languages, lack of programming logic in basic education, motivation, among others. Thus, this carries out a survey of the state of the art of existing and documented approaches in the literature, through a mapping of published works in the last five years (2012 to 2016) in two of Brazil's leading scientific computing platforms (CEIE and RENOTE), whose focus is to present solutions that address methodologies and tools that can be used in the different teaching modalities. As methodology was used the Systematic Review of Literature. As a result, it was found that, although studies still focus on higher education, in recent years there has been an increasing interest in programming teaching projects for children and teenagers, using gamification and tools such as Scratch. The results also demonstrate the growing interest of researchers in the search for approaches that provide better results in this area."
c,177,2020,"The traditional networks are facing difficulties in managing the services offered by cloud computing, big data, and the Internet of Things as the users have become more dependent on their services. Software-Defined Networking (SDN) has pulled enthusiasm in the integration process of technologies and function as per the user's requirements for both academia and industry, and it has begun to be embraced in actual framework usage. The emergence of SDN has given another idea to empower the focal programmability of the system. Because of the increasing demand and the scarcity of resources, the load balancing issue needs to be addressed efficiently to manage the incoming traffic and resources and to improve network performance. One of the most critical issues is the role of the controller in SDN to balance the load for having a better Quality of Service (QoS). Though there are few survey articles written on load balancing, there is no detail and systematic review conducted in load balancing in SDN. Hence, this paper extends and reviews the discussion with a taxonomy of current emerging load balancing techniques in SDN systematically by categorizing the techniques as conventional and artificial intelligence-based techniques to improve the service quality. The review also includes the study of metrics and parameters which have been used to measure the performance. This review would allow gaining more information on load balancing approaches in SDN and enables the researchers to fill the current research gaps."
d,178,2017,"Background: Logging practice is a critical activity in software development, which aims to offer significant information to understand the runtime behavior of software systems and support better software maintenance. There have been many relevant studies dedicated to logging practice in software engineering recently, yet it lacks a systematic understanding to the adoption state of logging practice in industry and research progress in academia. Objective: This study aims to synthesize relevant studies on the logging practice and portray a big picture of logging practice in software engineering so as to understand current adoption status and identify research opportunities. Method: We carried out a systematic review on the relevant studies on logging practice in software engineering. Results: Our study identified 41 primary studies relevant to logging practice. Typical findings are: (1) Logging practice attracts broad interests among researchers in many concrete research areas. (2) Logging practice occurred in many development types, among which the development of fault tolerance systems is the most adopted type. (3) Many challenges exist in current logging practice in software engineering, e.g., tradeoff between logging overhead and analysis cost, where and what to log, balance between enough logging and system performance, etc. Conclusion: Results show that logging practice plays a vital role in various applications for diverse purposes. However, there are many challenges and problems to be solved. Therefore, various novel techniques are necessary to guide developers conducting logging practice and improve the performance and efficiency of logging practice."
a,179,2018,"While conventional bilateral Single-Master/Single-Slave (SM/SS) teleoperation systems have received considerable attention during the past several decades, multilateral teleoperation is only recently being studied. Unlike an SM/SS system, which consists of one master-slave set, multilateral teleoperation frameworks involve a minimum of three agents in order to remotely perform a task. This paper presents an overview of multilateral teleoperation systems and classifies the existing state-of-the-art architectures based on topologies, applications, and closed-loop stability analysis. For each category, the review discusses control strategies used for various architectures as well as control challenges (e.g., closed-loop instability as a result of a delay in the communication network) for each methodology."
b,180,2017,"Cloud computing is a model to handle large scale distributed and grid computing by using virtualization. It empowered the client to acquire and manage the configurable shared pool of resources by their own or with minimum interference of service provider. However, Cloud services have heterogeneous Virtual Machines (VMs), with specified configuration and oscillate resources utilization, hosted on various servers situated different geographical location which may lead to imbalanced among resource and task scheduling. This results in poor performance, inefficient energy consumption, violation of service level agreement (SLAs), and instability in system. To overcome these issues, Load balancing plays vital role. The aim of load balancing algorithms is to improve the resource utilization, energy saving and deduction of carbon emission. In this paper, we have surveyed the nature inspired load balancing algorithm such as Particle Swarm Optimization (PSO), Ant Colony Optimization (ACO), Artificial Bee Colony (ABC), and Genetic Algorithm (GA), BAT. Our main focus in on Resource Utilization and Energy Saving."
c,181,2017,"Recently, there has been an incredible growth of recommender systems as well as proactive, context-oriented technologies, based on cloud services, ubiquitous computing and service-oriented architecture. This composition of techniques and technologies has made it possible to create intelligent support systems in areas with rapidly changing environment, like car driving. However, such systems are not yet widespread, and available prototypes, in most cases, are only useful for research trials, so their development remains an important issue. Thereby, this paper reviews the existing body of literature on recommender systems and related technologies in order to carry out their systematic analysis and draw the appropriate conclusions on the prospects for their development."
d,182,2020,"This paper has carried out a comprehensive review of the reliability analysis of microgrid. Survey papers on grid-connected microgrids have reported. In addition, the most applicable indices used in the reliability analyses of microgrids have been investigated. Different techniques used for coupling microgrids have been reported. In the same vein, a survey of different models used for comprehensive reliability analysis of microgrids is also reported. In a similar passion, the most frequent indices used in the reliability evaluations of microgrid have been defined. Different microgrids existing globally have also been presented in the paper. Finally, some future research topics in the area of this research have been identified."
a,183,2017,"Requirement Traceability (RT) is a significant method to analyze the effect of changes among various software development lifecycle artifacts. The RT not only improve the change management process but also assure the correctness and quality of the entire system. Although several RT techniques and models have been proposed by the researches, no research study is available yet to the best of our knowledge that analyze and summarize the latest RT developments. Therefore, in this article, a Systematic Literature Review (SLR) is performed to recognize 33 studies (2010-2017) in the area of RT. This leads to identify 7 models, 10 challenges and 14 tools. Subsequently, a comprehensive analysis of models and tools is presented. The findings of this study are highly valuable for the practitioners and researchers of the domain."
b,184,2018,"As the software applications have become important in our daily lives, the importance of the quality of the software has increased. To have guarantee about the quality of the software, it is essential for it to be tested properly. However, testing the software is very costly phase of the software development life cycle and it becomes more difficult with the growing dimension and complications involved with the software. Therefore, there is a need to make the testing process more effective. Testability improvement is one such method that helps in reducing the testing effort. Numerous methodologies for testability assessment have been anticipated in the literature. This paper provides the result of systematic review of the studies associated with software testability. In this paper, we review several journal and conference papers of software testability. In each study, researchers have used different approaches to measure software testability like controllability, observability etc. Thus this review focuses on various software testabilities theories and studies performed till date. In this paper, we have selected 55 primary studies after following a rigorous selection criteria. Results indicate that controllability and observability concept is widely explored for the measurement of software testability. Also, most of the work is based on object-oriented paradigm at the code level."
c,185,2019,"Accurate measurement of energy intake (EI) is important for estimation of energy balance, and, correspondingly, body weight dynamics. Traditional measurements of EI rely on self-report, which may be inaccurate and underestimate EI. The imperfections in traditional methodologies such as 24-hour dietary recall, dietary record, and food frequency questionnaire stipulate development of technology-driven methods that rely on wearable sensors and imaging devices to achieve an objective and accurate assessment of EI. The aim of this research was to systematically review and examine peer-reviewed papers that cover the estimation of EI in humans, with the focus on emerging technology-driven methodologies. Five major electronic databases were searched for articles published from January 2005 to August 2017: Pubmed, Science Direct, IEEE Xplore, ACM library, and Google Scholar. Twenty-six eligible studies were retrieved that met the inclusion criteria. The review identified that while the current methods of estimating EI show promise, accurate estimation of EI in free-living individuals presents many challenges and opportunities. The most accurate result identified for EI (kcal) estimation had an average accuracy of 94%. However, collectively, the results were obtained from a limited number of food items (i.e., 19), small sample sizes (i.e., 45 meal images), and primarily controlled conditions. Therefore, new methods that accurately estimate EI over long time periods in free-living conditions are needed."
a,186,2020,"The objective of this paper is to synthesize the digital interventions initiatives to fight against COVID-19 in Bangladesh and compare with other countries. In order to obtain our research objective, we conducted a systematic review of the online content. We first reviewed the digital interventions that have been used to fight against COVID-19 across the globe. We then reviewed the initiatives that have been taken place in Bangladesh. Thereafter, we present a comparative analysis between the initiatives taken in Bangladesh and the other countries. Our findings show that while Bangladesh is capable to take benefits of the digital intervention approaches, tighter cooperation between government and private organizations as well as universities would be needed to get the most benefits. Furthermore, the government needs to make sure that the privacy of its citizens are protected."
b,187,2020,"In this review, a methodical mapping of Blockchain-based research is performed across the multifaceted realm. The proposed research work is aimed at studying the prevailing research perspectives, obstacles, and upcoming considerations related to Blockchain from a technological point of view to underline the role of this disruptive technology in the current sci-tech ecosystem. For this purpose, the hypothetical fundamentals of countless work published in reputed scientific publications in the past ten years, are integrated into this paper. Following the structured, methodical review and thorough study of the recognized literature, an extensive taxonomy of Blockchain-based research in various domains like security, usability, privacy, smart contracts, throughput, latency, wasted-resources, broadcast protocol, and trustworthiness is being presented, and key presentations, directions, and originating research areas are adopted. Developing on the evaluations, various unexplored research topics are discovered and future prospecting indications that are expected to be of substantial value both for theoreticians and professionals."
c,188,2019,"Cloud storage offers a considerable efficiency and security to the user's data and provide high flexibility to the user. The hackers make attempt of several attacks to steal the data that increase the concern of data security in cloud. The Third Party Auditing (TPA) method is introduced to check the data integrity. There are several TPA methods developed to improve the privacy and efficiency of the data integrity checking method. Various methods involved in TPA, have been analyzed in this review in terms of function, security and overall performance. Merkel Hash Tree (MHT) method provides efficiency and security in checking the integrity of data. The computational overhead of the proof verify is also analyzed in this review. The communication cost of the most TPA methods observed as low and there is a need of improvement in security of the public auditing."
d,189,2019,"This paper reviews the most recent literature on UTAUT/ UTAUT 2 (Unified Theory of Acceptance, and Use of Technology) by focusing on the findings and recommended future research. To meet this aim, the authors conduct an in-depth analysis of the current literature by using the systematic review method. This research paper is conceptual in origin, and it was supplemented by analysis 65 papers from various websites which hosted scientific journals. These papers employed UTAUT and UTAUT 2 as a baseline framework. The researchers focused on findings in these papers and on the core constructs of UTAUT used to predict behavioral intentions. The results from the previous studies confirmed that the four core constructs of UTAUT/UTAUT2 contributed to understanding of the behavioral intention to accept / adopt or use the technology and the performance expectancy (PE) has significantly correlated with technology adoption. Moreover, the previous studies show that UTAUT 2 has been more explanatory in predicting behavioral intentions and should be used for the future works in adopting any new technology."
a,190,2020,"Building Information Modeling (BIM) employs data-rich 3D CAD models for large-scale facility design, construction, and operation. These complex datasets contain a large amount and variety of information, ranging from design specifications to real-time sensor data. They are used by architects and engineers for various analysis and simulations throughout a facility's life cycle. Many techniques from different visualization fields could be used to analyze these data. However, the BIM domain still remains largely unexplored by the visualization community. The goal of this article is to encourage visualization researchers to increase their involvement with BIM. To this end, we present the results of a systematic review of visualization in current BIM practice. We use a novel taxonomy to identify main application areas and analyze commonly employed techniques. From this domain characterization, we highlight future research opportunities brought forth by the unique features of BIM. For instance, exploring the synergies between scientific and information visualization to integrate spatial and non-spatial data. We hope this article raises awareness to interesting new challenges the BIM domain brings to the visualization community."
b,191,2019,"Combining tiny sensors and wireless communication technology, wireless body area network (WBAN) is one of the most promising fields. In other words, the development of wireless body area network has accelerated due to the rapid development of wireless technology. This article presents a detailed survey as well as a comparative study on wireless body area networks (WBANs). Basically, the paper aims to study the significant characteristics, issues and challenges with WBAN in various fields majorly, in medical application. Moreover, recurrent technologies used in WBANs are the ones most focussed upon during the findings. Also, the recent trends and future research scope have been addressed in this article."
c,192,2020,"Software quality prediction is the process of evaluating the software developed for various metrics like defect prediction, bug localisation, effort estimation etc. To evaluate these metrics a myriad of techniques have been developed in the literature, from manual assessment to application of machine learning and statistical testing. These methodologies, however, had lower accuracy in determining SQPMs due to their inability to model the complex relationships in the training data. With the wide emergence of deep learning, not only has the accuracy of the pre-existing models enhanced, but it has also opened doors for new metrics that could be evaluated and automated. This study performs a systematic literature review of research papers published from January 1990 to January 2019 that used deep learning to evaluate software quality prediction metrics (SQPM). The paper identifies 20 primary studies and 7 categories of application of deep learning in SQPM. Models using deep learning techniques significantly outperform other traditional methodologies in almost all studies. The concept and external threats to the models are limited, however the time taken to train these models is large. The techniques, currently predominantly applied for defect prediction, have shown promising results in other diverse software engineering fields like code search and effort estimation by modeling the source code efficiently. There is, hence, scope for incorporating deep learning further with pragmatic use and diverse application. The need to find scalable solutions, however, still persists."
d,193,2020,"The recent state of the art innovations in technology enables the development of low-cost sensor nodes with processing and communication capabilities. The unique characteristics of these low-cost sensor nodes such as limited resources in terms of processing, memory, battery, and lack of tamper resistance hardware make them susceptible to clone node or node replication attack. The deployment of WSNs in the remote and harsh environment helps the adversary to capture the legitimate node and extract the stored credential information such as ID which can be easily re-programmed and replicated. Thus, the adversary would be able to control the whole network internally and carry out the same functions as that of the legitimate nodes. This is the main motivation of researchers to design enhanced detection protocols for clone attacks. Hence, in this paper, we have presented a systematic literature review of existing clone node detection schemes. We have also provided the theoretical and analytical survey of the existing centralized and distributed schemes for the detection of clone nodes in static WSNs with their drawbacks and challenges."
a,194,2019,"Code cloning refers to the duplication of source code. It is the most common way of reusing source code in software development. If a bug is identified in one segment of code, all the similar segments need to be checked for the same bug. Consequently, this cloning process may lead to bug propagation that significantly affects the maintenance cost. By considering this problem, code clone detection (CCD) appears as an active area of research. Consequently, there is a strong need to investigate the latest techniques, trends, and tools in the domain of CCD. Therefore, in this paper, we comprehensively inspect the latest tools and techniques utilized for the detection of code clones. Particularly, a systematic literature review (SLR) is performed to select and investigate 54 studies pertaining to CCD. Consequently, six categories are defined to incorporate the selected studies as per relevance, i.e., textual approaches (12), lexical approaches (8), tree-based approaches (3), metric-based approaches (7), semantic approaches (7), and hybrid approaches (17). We identified and analyzed 26 CCD tools, i.e., 13 existing and 13 proposed/developed. Moreover, 62 open-source subject systems whose source code is utilized for the CCD are presented. It is concluded that there exist several studies to detect type1, type2, type3, and type4 clones individually. However, there is a need to develop novel approaches with complete tool support in order to detect all four types of clones collectively. Furthermore, it is also required to introduce more approaches to simplify the development of a program dependency graph (PDG) while dealing with the detection of the type4 clones."
b,195,2018,"As we know that number of devices connected through internet are growing rapidly, based on an estimation the number of active internet connections will be seven connections per human by the year 2020. These smart objects connect and communicate with each other and create IoT environment. Integration of such smart objects is not an easy task. Although different challenges and issues are associated with IoT, but due to its great applicability researchers have proposed a number of solutions to overcome any difficulty. One of the major concern in IoT environment is security. Various frameworks and solutions were proposed to get over the security issue. In this research paper, we are performing a systematic review on different security frameworks and the major focus of this research paper is on trust based security framework."
c,196,2017,"Presently, educational institutions compile and store huge volumes of data, such as student enrolment and attendance records, as well as their examination results. Mining such data yields stimulating information that serves its handlers well. Rapid growth in educational data points to the fact that distilling massive amounts of data requires a more sophisticated set of algorithms. This issue led to the emergence of the field of educational data mining (EDM). Traditional data mining algorithms cannot be directly applied to educational problems, as they may have a specific objective and function. This implies that a preprocessing algorithm has to be enforced first and only then some specific data mining methods can be applied to the problems. One such preprocessing algorithm in EDM is clustering. Many studies on EDM have focused on the application of various data mining algorithms to educational attributes. Therefore, this paper provides over three decades long (1983-2016) systematic literature review on clustering algorithm and its applicability and usability in the context of EDM. Future insights are outlined based on the literature reviewed, and avenues for further research are identified."
d,197,2020,"Finger print recognition system is a security concern to enter through our nger print. The nger print is identied by nger print scanner like Microsoft Fingerprint Reader. Finger print recognition is regarded as the most reliable and accurate biometric identication system available. Every person is identied by their individual attendance, lock opened through their own unique nger prints. It is most actively studied as a biometric technology. The ngerprint recognition problem is regarded as ngerprint verication and ngerprint identication. The nger print pattern matching is widely used in areas like security lock door system, house entries, mobile screen locks etc,. The goal of this paper is to exhibit the journey of modern fingerprint recognition systems with their merits and demerits."
a,198,2020,"Fusion technologies have rapidly evolved. These technologies are normally customized according to the needs of domains. Despite a large number of publications on intelligence fusion applications for various domains, they are scattered. The aim of this review is to present the state of the art for intelligence fusion applications within a specific domain. We identified three major domains for the purpose, namely robotics, military, and healthcare, during the initial process of the systematic review. These three domains are always in need of superior intelligence. Articles were searched mainly in IEEE Xplore. We limit the range of publications to the year 2014 to 2019, to focus on the most recent publications. We adopt the Preferred Reporting Items for Systematic Reviews and Meta-Analyses (PRISMA) protocol to screen, filter and evaluate qualities of each retrieved article. As a result, we retrieved 675 articles at the initial stage of the search, we conducted screening and filtering process and reviewed 153 articles potential articles, and finally, we excluded 36 articles as they do not comply with our quality assessment criteria. Only 117 articles are included. The results of this study are a list of classified applications within the domains and a number of relevant techniques or approaches used in each classified application. The finding of this review showed that the most published works for the use of intelligence fusion are mainly applications in the robotics domain, where mostly used techniques are Kalman Filter and its variants. Outcomes of this study can be a guideline or an insight for researchers to further develop and implement in this field."
b,199,2020,"Bio-metric frameworks are getting to be progressively important, since they are more reliable and proficient for identity confirmation. One such biometric is gait. The pattern by which an individual walks is mentioned as gait. It's a locomotion that's achieved through the movement of a person's limb. Unlike several approaches gait is a behavioral biometric, that is taken into consideration for user authentication as it shows distinct patterns for every individual. Also, less obtrusion of user has made this biometric method to be more advantageous compared to others. During this survey we tend to concentrate on varied gait approaches, applications and various machine learning techniques which will be used for classification of gait features and its applications."
c,200,2019,"This paper gives a review of the literature on the application of Hidden Markov Models in the field of sentiment analysis. This is done in relation to a research project on semantic representation and the use of probabilistic graphical models for the determination of sentiment in textual data. Relevant articles have been analyzed that correspond mainly to the certain variations of the implementation of HMM and a variety of use cases for the purpose of sentiment classification. Finally, this review presents the grounds for future works that seek to develop techniques for semantic text representations implemented with probabilistic graphical models (Hidden Markov Models) or that through a combination scheme allow for superior classification performance."
d,201,2020,"Techniques predicting the type of diseases affecting plants in their lifetime will be of immense help to agriculturists. This article throws light upon such techniques in the form of a survey that had been carried out comprehensively covering various image based plant leaf diseases. Such diseases are mostly grievous and they strike at any part of the plant. There are huge accumulated losses due to such diseases that bring down the productivity and increase the economic losses in the agricultural industry. Agriculture industry needs to sustain and evolve from such obstacles to be highly profitable. This can be done by precisely monitoring the health and detecting the diseases at appropriate stages of the plant's life time. Technology has spread its wings in every field of day to day life but still its reach in the field of agriculture is not up to the mark. Agriculture industry is still thriving on outdated technical methodologies. Improper diagnosis of plant disease may lead to huge losses in terms of production, time, cost and product quality. The condition of the plant needs to be tracked throughout its growing stages leading to successful cultivation. As part of technological innovation, researchers had been applying the image processing techniques for monitoring as well as diagnosing the plant diseases in its various stages. Appropriate machine learning algorithms are being designed and applied for precisely identifying the various infections on plants throughout its life cycle and the type of treatment that can be afforded for overcoming loss."
a,202,2020,"Robotics is one of the many tools that is making a substantial difference as the world is experiencing the fourth industrial revolution. To ease control over this engineering marvel substantially, Reinforcement Learning (RL) has paved its way in recent years quite remarkably. RL enables robots to become self-aware towards carrying out a specific task followed by user operations. For decades of rigorous endeavor, this research field has gone through numerous groundbreaking developments and it will be the same for the coming days. Therefore, this paper steps in to enlighten the scientific community with a systemic review of the published research papers within the past decade. The bibliographic data that is extracted from the papers are analyzed using an automated tool named Vosviewer with respect to some parameters. Substantial excerpts from the most influential papers are highlighted in this work. Furthermore, this paper points out the global research practice in this field. The paper also generates some intriguing questions and answers them in regards to the research topic. After reading this paper, future researchers will have a firm idea in the RL-based robotics and will be able to incorporate in their own research."
b,203,2018,"Regression testing aims at testing a system under test (SUT) in the presence of changes. As a SUT changes, the number of test cases increases to handle the modifications, and ultimately, it becomes practically impossible to execute all of them within limited testing budget. Test suite reduction (TSR) approaches are widely used to improve the regression testing costs by selecting representative test suite without compromising effectiveness, such as fault-detection capability, within allowed time budget. The aim of this systematic review is to identify state-of-the-art TSR approaches categories, assess the quality of experiments reported on this subject, and provide a set of guidelines for conducting future experiments in this area of research. After applying a two-facet study selection procedure, we finalized 113 most relevant studies from an initial pool of 4230 papers published in the field of TSR between 1993 and 2016. The TSR approaches are broadly classified into four main categories based on the literature including greedy, clustering, search, and hybrid approaches. It is noted that majority of the experiments in TSR do not follow any specific guidelines for planning, conducting, and reporting the experiments, which may pose validity threats related to their results. Thus, we recommend conducting experiments that are better designed for the future. In this direction, an initial set of recommendations is provided that are useful for performing well-designed experiments in the field of TSR. Furthermore, we provide a number of future research directions based on current trends in this field of research."
c,204,2018,"The general approach of the work described is to develop a Systematic Review to assist health professionals, specifically Physiotherapists, collecting data about interactive environments used to assist in the treatment of injuries from stroke. Although the area is well studied, it is necessary to take into account the lack of studies that mix the development of the environments, as well as the interactivity with the patient. In this work a Systematic Review was conducted in which 13 scientific papers published on virtual and interactive environments were studied until June 2017. The main essence of this study is to provide technologists, physicians and physiotherapists a better and continuous approach to offer the patient, ways in which one does not have to sacrifice oneelf or consider such monotonous procedures."
d,205,2019,"Load forecasting is always required for the planning and extension of power system. With the emergence of smart grid, forecasting of load and its management has been a primal concern for a researcher. Load forecasting has been challenging not only for developing countries but also for developed and industrialized nations. The main problems for developing nations are missing necessary data, appropriate load forecasting models and required institutions, these limitations are somewhat less serious for developed nations. Due to limitations in the model structure and missing data forecasted energy demands are often found to deviate from the actual demands. This paper contains a systematic review on the methods of load forecasting which is done on the basis of 126 research papers and the best methods for load forecasting on the basis of time, inputs, outputs and error type have been determined. The methods compared are time series analysis and machine learning algorithms. This meta-analysis has been defined to help researchers to take an effective decision by choosing the right model from their problem."
a,206,2018,"Educational Robotics (ER) has revealed several benefits in the educational context, not only helping the teaching of disciplines, but also making possible the development of several abilities, such as teamwork, problem-solving, and creativity. Among various robotics kits, LEGO  Robotics has been shown one of the best results considering some evaluated criteria (modularity level, hardware, curriculum, price, etc.). Some studies analyze the teaching practices, some compare technologies, and others evaluate the kits in a pedagogical way. However, it is essential to investigate all these contexts together in order to improve the impact produced by the ER in education and to know the best teaching practices associated with the most powerful technologies. The objective of this Research Full Paper is to identify: a) environments and programming languages adopted in the LEGO  Robotics context, b) educational practices applied during classes based on LEGO  Robotics, and c) the educational levels in which robotics has been applied with positive results. To achieve these goals, we planned and carried out a systematic review of the literature. Our main findings are: a) the most widely used environment and programming language are LabVIEW along with the LEGO 's block-based programming language, b) we identified LEGO  Robotics is used for teaching programming, interdisciplinary contents, participation in tournaments, robotics, and computational thinking, c) LEGO  Robotics is used with success by students of different levels, such as K12, undergraduate, and graduated. Finally, we discuss some problems and limitations related to ER and point out that there is no standardization of teaching practices or methodologies for evaluating results, indicating that more research is needed to find the best scenario regarding technologies, methods, and target audience."
b,207,2018,"Vehicle detection and classification from a moving platform is an important factor in Intelligent Transportation System (ITS). Vehicle identification and classification provide input to traffic control measures and traffic management systems. Vehicle identification and classification in congested traffic and busy roads is still facing challenges due to a number of factors like different types of vehicles, the vehicle with almost similar features having different class etc. Improving Transportation System in big cities is facing more challenges due to a number factor like traffic congestion, lane problems. This review paper is on vehicle detection and classification to address the work done in this field and feature demands of classification and identification of vehicles."
c,208,2017,"This paper presents a systematic review of evidence based studies on attention detection and measurement in virtual reality (VR) based learning intervention for children with Autism Spectrum Disorder (ASD). We carried out a search from three different large databases for articles between January 2008 and May 2017. The objective of this paper is to review learning intervention studies featuring measures of attention as an outcome variable for children with ASD in a virtual reality environment thereby addressing four specific questions: what are the types of attention studied in a virtual reality environment for children with ASD, the attentional behaviors, the technology used in detecting and measuring the attentional behaviors and the synthesis of empirical evidence across the reviewed studies. In the context of (VR) environment, there are five types of attention studied in the reviewed articles where nine attentional behaviors were targeted with six different technologies and strategies. The synthesis of the empirical studies reviewed showed that two out of five types of attention are always targeted for attention evaluation, four out of the nine types of attentional behaviors are commonly studied, two out of the six technologies are mostly used and two out of the six strategies of attention assessment are popularly used. The synthesis outcome of this systematic review can serve as an evidence base for researchers who are interested in attention assessment of children with ASD in a virtual reality environment."
d,209,2017,"B-cell conformational epitope identification is the crucial issue in vaccinology. Limitation on experimental methods in the biological side, dataset and availability of computational resources opens a chance on developing prediction method which can accelerate epitope identification. A number of methods have been developed but their performance is still medium. Epitope prediction is a knowledge-based method. Presenting the statistical or computational based epitope characteristics together with epitope prediction method will facilitate the newcomer on identifying importance feature, improve the existing feature and propose the new feature. To reach the goal of the review, the research papers are collected from both epitope analysis research and epitope prediction methods research. The prediction methods are evaluated on what characteristics of epitope have been implemented on feature representation and are shown in a mapping table."
a,210,2020,"Wi-Fi has been widely accepted in today's generation since it provides liberty to gain access to the network without being physically bonded. Some institutions believe this results in an increase in service and productivity, while reducing the cost of physical wiring; as users bring their devices such as laptops and tablets. With embracing the benefits of Wi-Fi, potential cyber threats can arise through this usage. This paper discusses the greater concern on security issues arising and assessment while embracing the benefits of Wi-Fi. Existing literature was used to evaluate the vulnerability assessment mechanisms available and propose recommendations that educational environments can further adopt, in addition to existing measures to better protect its users."
b,211,2019,"In a Technological financial world where everything is progressing very fast and the amount of financial transactions through the automatic teller machines (ATM) on a daily basis is increasing. In parallel to these developments, fraudulent attacks and identity theft targeting to access the ATM and steal money or use bank accounts. Those attacks are always possible through the weak security points in the ATMs. The main aim of this study is to investigate and categorize the different approaches taken to overcome those weak security points by using biometric data along with some traditional methods. Accordingly, a systematical mapping was conducted by focusing on research studies published in the years 2004 to 2019 and addressing to the ATMs and biometric security technologies. After an intense investigation, 23 different systems were investigated using different single and multi-biometric models which make use of 8 different biometric technologies and 5 different traditional technologies as key components. As a conclusion from this research that researchers are leaning towards using biometric data in enhancing the security of the ATMs especially in the past 4 years more than they did before."
c,212,2019,"Software visualization (SV) allows us to visualize different aspects and artifacts related to software, thus helping engineers understanding its underlying design and functionalities in a more efficient and faster way. In this paper, we conducted a tertiary systematic literature review to identify, classify, and evaluate the current state of the art on software visualization from 48 software visualization secondary studies, following three perspectives: publication trends, software visualization topics and techniques, and issues related to research field. Hence, we summarized the main findings among popular sub-fields of SV, identifying potential research directions and fifteen shared recommendations for developers, instructors and researchers. Our main findings are the lack of rigorous evaluation or theories support to assess SV tools effectiveness, the disconnection between tool design and their scope, and the dispersal of the research community."
d,213,2019,"Information and Communication Technology (ICT) presents immense opportunities to SMEs by providing platforms to share resources and to collaborate better. The umbrella concept that explains the coordinated effort of people to share resources and collaborate through digital platforms is generally known as the Sharing Economy (SE). This study investigated SE research trends and gaps from the context of SMEs. The exploration analyzed six different SE aspects from 77 previous SE studies in the context of SMEs. These aspects included SE forms, values of SE for SMEs, SMEs sectors, the context of nations, geographic origin of data and cases, and methodological."
a,214,2020,"This paper presents a systematic review of the literature to investigate best practices for developing digital games for people with visual impairments. The execution of the search protocol that focused on papers published between 2004 and 2019, 17 papers were obtained from two online databases. The selected papers present information about the application of accessibility features that were tested in a minimum viable product applied to people with visual impairments. As a result, it was found that most papers focus on navigation systems, which is a feature used in most digital games, and all of them make use of sound effects. In most of the researched papers, audio is the main instrument to assist the visually impaired player. The development features presented in the systematic review can serve as a basis for developers in the field. Finally, there is still little content in the literature on accessible features available in the context of digital games, which makes it an indispensable research field to explore."
b,215,2018,"This study aimed to evaluate the clinical effects of acupoint stimulation therapy on patients with obstructive sleep apnea hypopnea syndrome (OSAHS). Methods: A systematic literature search of electronic databases (PubMed, Embase, CENTRAL and Chinese database) was conducted for randomized controlled trials comparing acupoint stimulation with other treatments in OSAHS patients. Literature collected in this paper was published up to May 2016, and random effects meta-analyses were performed by RevMen 5.3 software. Results: 703 patients from 10 trials were included. Comparison between acupoint stimulation and conventional treatment to OSAHS patients, showed significant differences in AHI ([WMD=-9.71, 95% CI (-11.63, -7.78), P<;0.00001]), SaO2min ([WMD=4.20, 95% CI (2.26, 6.14), P<;0.00001]), and Epworth Sleepiness Scale (ESS) scores ([WMD=- 3.99, 95%CI (-5.83,-2.15), p<;0.0001]). However, significant difference between acupoint stimulation and CPAP treatment to OSAHS was not observed. Conclusion: Evidence from this study suggests that stimulate acupoints therapy is effective in improving AHI, SaO2min and ESS in patients with OSAHS."
c,216,2019,"To unleash the great potential of the Internet of Things (IoT), it is critical to facilitate the creation and operation of IoT systems across IoT, edge and cloud infrastructures with vast heterogeneity, scalability and dynamicity. What is the current landscape of the existing approaches and tools that attempt to cope with this complexity? The work presented in this paper contributes to this picture. This paper presents the results of our systematic literature review (SLR) on research approaches and tools for the deployment and orchestration of IoT systems (DEPO4IoT). From thousands of relevant publications, we systematically identified and reviewed seventeen (17) primary studies for data extraction and synthesis to answer our predefined research questions. The results of our SLR show the technical details of the primary DEPO4IoT studies. A main finding is that most approaches do not properly support software deployment and orchestration at the tiny IoT device level. Moreover, there is a lack in terms of properly addressing the trustworthiness aspects in approaches for IoT systems deployment and orchestration. In this paper, we suggest some potential research directions to address the gaps found."
d,217,2018,"The diagnosis of most lung cancer patients happen only when it is advanced, where the curative therapy is not any more a choice. Lung cancer has more death rates than other prevalent cancers like prostate, breast and skin around the globe. Its mortality rate can be reduced to a certain extend by the earlier detection. It is also very important to know the different types of detection methods and its effectiveness. This review centers upon the likelihood of diagnosing lung cancer at its earlier stage by utilizing different biomarkers like protein biomarkers and exhaled breath analysis and imaging procedures like CT scan imaging."
a,218,2020,"Cyber-security is the practice of protecting computing systems and networks from digital attacks, which are a rising concern in the Information Age. With the growing pace at which new attacks are developed, conventional signature based attack detection methods are often not enough, and machine learning poses as a potential solution. Adversarial machine learning is a research area that examines both the generation and detection of adversarial examples, which are inputs specially crafted to deceive classifiers, and has been extensively studied specifically in the area of image recognition, where minor modifications are performed on images that cause a classifier to produce incorrect predictions. However, in other fields, such as intrusion and malware detection, the exploration of such methods is still growing. The aim of this survey is to explore works that apply adversarial machine learning concepts to intrusion and malware detection scenarios. We concluded that a wide variety of attacks were tested and proven effective in malware and intrusion detection, although their practicality was not tested in intrusion scenarios. Adversarial defenses were substantially less explored, although their effectiveness was also proven at resisting adversarial attacks. We also concluded that, contrarily to malware scenarios, the variety of datasets in intrusion scenarios is still very small, with the most used dataset being greatly outdated."
b,219,2020,"Autonomous vehicles (AVs) may have a transformative influence on transportation systems. AV use is expected to reduce crash, transport costs, and land use. However, the net effects of the autonomous vehicle are unknown because there are no fully-autonomous vehicles in reality. The simulation systems are indispensable to be built up not only for cost but also for obtaining massive data. An agent-based simulation is capable of simulating the entities in a complex system, such as the AV system. Considering the potential of agent-based models to complement existing models and the difficulty to understand, to duplicate and to compare different agent-based models, this paper review existing agent-based modeling of autonomous vehicles with the help of ODD protocol, which has been applied in the review of agent-based land-use change models and agricultural policy evaluation. The significant variables that researchers have taken into consideration in sensitivity analysis and in the different scenarios are explored in the paper. Fleet size is the primary variable researchers considered, and we also discussed the probable causes of different AV replacement rates. We have assessed the quality of reviewed articles to gain more acceptance from the agent-based modeling community. Most of the reviewed papers follow the ODD protocol of agent decision making. However, in order to improve the level of modeling transparency, the executable and related data shall be available, at least the data source and the programming language/simulation platform should be provided. Several recommendations for further research are also presented."
c,220,2018,"Since ERP solution facilitates has changed in business environment and also has helped in supporting managerial level to make decision. Therefore, ERP system is important strategy for organization's survival. One of the critical issues that affect a successful ERP implementation is the implementation method. In previous research, it showed that many organizations had faced complexity when they were implementing ERP in order to meet their requirements. In other cases, the ERP systems are unable to provide solution which is a simplicity of the necessary business processes. The ERP implementation complexity is one of the factors that lead the failures. Thus, here, we intend to propose an agile approach to reduce complexity and eventually our intention is to improve ERP implementing to become more successful. In this study, we are employing a systematic literature review (SLR) approach to identify the principles factors of the agile methods for ERP implementation. In result of employing this SLR method, we found that agile values and principles are correlated in ERP implementation complexity."
d,221,2020,"Machine learning (ML) techniques are changing both the offensive and defensive aspects of cybersecurity. The implications are especially strong for privacy, as ML approaches provide unprecedented opportunities to make use of collected data. Thus, education on cybersecurity and AI is needed. To investigate how AI and cybersecurity should be taught together, we look at previous studies on cybersecurity MOOCs by conducting a systematic literature review. The initial search resulted in 72 items and after screening for only peer-reviewed publications on cybersecurity online courses, 15 studies remained. Three of the studies concerned multiple cybersecurity MOOCs whereas 12 focused on individual courses. The number of published work evaluating specific cybersecurity MOOCs was found to be small compared to all available cybersecurity MOOCs. Analysis of the studies revealed that cybersecurity education is, in almost all cases, organised based on the topic instead of used tools, making it difficult for learners to find focused information on AI applications in cybersecurity. Furthermore, there is a gab in academic literature on how AI applications in cybersecurity should be taught in online courses."
a,222,2019,"Cardiovascular diseases currently pose the highest threat to human health around the world. Proper investigation of the abnormalities in heart sounds is known to provide vital clinical information that can assist in the diagnosis and management of cardiac conditions. However, despite significant advances in the development of algorithms for automated classification and analysis of heart sounds, the validity of different approaches has not been systematically reviewed. This paper provides an in-depth systematic review and critical analysis of all the existing approaches for automatic identification and classification of the heart sounds. All statements on the Preferred Reporting Items for Systematic Reviews and Meta-Analyses 2009 Checklist were followed and addressed thoroughly to maintain the quality of the accounted systematic review. Out of 1347 research articles available in the academic databases from 1963 to 2018, 117 peer-reviewed articles were found to fall under the search and selection criteria of this paper. Amongst them: 53 articles are focused on segmentation, 72 of the studies are related to the feature extraction approaches and 88 to classification, and 56 reported on the databases and heart sounds acquisition. From this review, it is clear that, although a lot of research has been done in the field of automated analysis, there is still some work to be done to develop robust methods for identification and classification of various events in the cardiac cycle so that this could be effectively used to improve the diagnosis and management of cardiovascular diseases in combination with the wearable mobile technologies."
b,223,2019,"Amplitude-integrated electroencephalography (aEEG) is a simplified method for long-term, continuous, and bedside monitoring of brain activity. While conventional Electroencephalography (EEG) is the gold standard of assessing brain function, aEEG is easy to operate and allows bedside interpretation of brain activity by health care providers without extensive knowledge of neurophysiology. aEEG is increasingly applied in neurological monitoring in neonates, especially in the neonatal intensive care unit (NICU). To a growing extent, researchers and clinicians are convinced that aEEG provides valuable clinical information and can be used to assess the severity of neonatal encephalopathy. Meanwhile, to digitalize the aEEG transformation process and automate the interpretation process, different algorithms have been proposed in the last decades. This paper provides a comprehensive review of aEEG for neonatal monitoring from both clinical and technological perspectives. The paper first reviews the clinical applications of aEEG and discusses the merits and demerits of neonatal aEEG monitoring in terms of the assistance of the treatment and prognosis of cerebral diseases like hypoxic-ischemic encephalopathy (HIE), seizure and so on. And then furthermore, the algorithms to transform EEG into aEEG and the algorithms for aEEG interpretation like the automatic classification of aEEG tracing, automatic seizure detection of aEEG, etc. are reviewed."
c,224,2018,"This Research Full Paper presents a review of the literature related to novices' interest in learning to code. Students' interest in this topic has been mentioned in the Computer Science Education (CSE) field since the 1990s. Although it got the attention of computing faculty and shaped a wide number of proposals, students' disinterest remains a complex struggle in Computer Science (CS) courses. On the other hand, we still have little knowledge about what theoretical concepts and methods of research are adopted by researchers who study interest, particularly among novices in programming. As an attempt to close this gap, we carried out a systematic review process to identify frequent assumptions and methodological decisions aiming to establish a sense of what happened in the last seventeen years in this research field. Based on our review, the findings revealed that although a high number of contributions is propagated, most of them are based on lesson learned reports that fail in producing reliable evidences to substantiate the effectiveness of some proposals. The analysis of the 36 publications considered relevant to our systematic review revealed that there is not a convergent understanding between CSE researchers of what is interest and how to measure. Not many of them study interest based on specific theories or observe how students' interest changes over time. So, although we have found many publications referring to practical and experimental applications, we could see that this research area is still fragmented and led by a bit of uncertainty about what really can sustain novices more interested in learning coding."
d,225,2018,"A Systematic Literature Review (SLR) allows us to combine and analyze data from multiple (published and unpublished) studies. Though it provides a complete and comprehensive empirical evidence of an area of interest, the results we usually get from the data synthesis phase of an SLR include huge tables and graphs and thus, for users, it is a tedious and time-consuming job to get the required results. In this work, we propose to semi-automate some steps which can be used to fetch the information from an SLR, beyond the traditional tables, graphs, and plots. The automation is performed using Semantic Web technologies like ontology, Jena API and SPARQL queries. The Semantic Web, also called Web 3.0, provides a common framework and thus allows us to share and re-use the data across the applications and enterprises. It can be used to integrate, extract, and infer the most relevant data required by the users, which are hidden behind the huge information on the Web. We also provide an easy-to-use user interface in order to allow users to perform different searches and find their required SLR results easily and quickly. Finally, we present the results of a preliminary user study performed to analyze the amount of time users need to extract their required information, both via the SLR tables and our proposal. The results revealed that with our system the users get their required information in less time compared to the manual system."
a,226,2019,"The Internet of Things (IoT) aims at connecting things to the Internet in a peer-to-peer paradigm for data collecting and data sharing in our daily life. A blockchain is an immutable append-only ledger maintained by a peer-to-peer network, where the whole network needs to reach a consensus on the transactional data stored on the ledger. With the decentralization nature, the design of IoT and blockchain aligns with each other well. Blockchain has been integrated with the IoT to solve the existing IoT problems. Our research focuses on analyzing the solutions proposed in academia and the methodologies used to integrate blockchain with the IoT. Through conducting a systematic literature review (SLR) on peer-reviewed, published articles on blockchain-based solutions for IoT, we gather the knowledge on current technical approaches implemented to integrate blockchain into the IoT. Majority of the research in this space is either at a conceptual level or at a very early stage. However, we only found 35 published papers with the real implementation of blockchain in the IoT platforms. We elicit the challenges of the IoT that were being addressed, and the detailed design of the blockchain-based solutions from two perspectives, namely data management and thing management. The evaluation methods and metrics used by those works are also being recorded and analyzed. In addition to the analysis of the literature, we provide our insights on improving the existing solutions and research methodology based on our expertise and experience on the blockchain."
b,227,2020,"As the complexity of both design and processing increase for advanced FinFET technology, defect metrology will continue to provide the best strategies for Defect Review Scanning Electron Microscopy (DR-SEM) and Automatic Defect Classification (ADC). The precise defect location navigation, and the accuracy of random and systematic defect classification can be improved by introducing Computer-Aided Design (CAD) into DR-SEM and ADC processes. The Design Based ADC (DBA) not only differentiates systematic and random defect on CAD for yield control but also determines the precise defect locations for weak point analysis. We will present a method for the implementation of weak point CAD for DR-SEM review and ADC through a case study of a BEOL CMP layer of an advanced FinFET process demonstrating its benefit to defect analysis and control."
c,228,2019,"Anomaly detection has attracted considerable attention from the research community in the past few years due to the advancement of sensor monitoring technologies, low-cost solutions, and high impact in diverse application domains. Sensors generate a huge amount of data while monitoring the physical spaces and objects. These huge collected data streams can be analyzed to identify unhealthy behaviors. It may reduce functional risks, avoid unseen problems, and prevent downtime of the systems. Many research methodologies have been designed and developed to determine such anomalous behaviors in security and risk analysis domains. In this paper, we present the results of a systematic literature review about anomaly detection techniques except for these dominant research areas. We focus on the studies published from 2000 to 2018 in the application areas of intelligent inhabitant environments, transportation systems, health care systems, smart objects, and industrial systems. We have identified a number of research gaps related to the data collection, the analysis of imbalanced large datasets, limitations of statistical methods to process the huge sensory data, and few research articles in abnormal behavior prediction in real scenarios. Based on our analysis, researchers and practitioners can acquaint themselves with the existing approaches, use them to solve real problems, and/or further contribute to developing novel techniques for anomaly detection, prediction, and analysis."
d,229,2019,"With the rapid development of computer and communication technology, anticipatory computing has been identified as one of the most important factors affecting human behavioral change. The future of anticipatory computing will not be bright if it fails to provide useful help to human life and work. Anticipatory computing applied to behavioral change intervention (BCI) is full of challenges and is a research topic of increasing interest and importance. This paper provides an overview of the concept of anticipatory computing, BCI, as well as anticipatory computing for the BCI and offers a multistage literature analysis. Also, a systematic analytical framework articulated from the existing literature is presented to reveal the progress and details of anticipatory computing for the BCI. This framework is divided into four dimensions: 1) sensing and context inferring; 2) context prediction; 3) behavioral guidance and intervention, and; 4) application. Based on our literature analysis, 11 elements of anticipatory computing for BCI are identified and discussed in terms of principles, enablers, and activities. Afterward, contributions and possible future directions for research are summarized at the end of this paper."
a,230,2019,"Advancement of consensus protocols in recent years has enabled distributed ledger technologies (DLTs) to find its application and value in sectors beyond cryptocurrencies. Here we reviewed 66 known consensus protocols and classified them into philosophical and architectural categories, also providing a visual representation. As a case study, we focus on the public sector and highlighted potential protocols. We have also listed these protocols against basic features and sector preference in a tabular format to facilitate selection. We argue that no protocol is a silver bullet, therefore should be selected carefully, considering the sector requirements and environment."
b,231,2018,"Sustainability is emerging as a main consideration throughout the industrial world due to the environmental pollution and degradation happening in a major scale as a result of industrial wastes while lean management is becoming a popular management tool in minimizing waste. Logistics industry contributes for these issues due to the wastes released in a considerable amount. Experts have highlighted that implementing lean principles in parallel to green concepts is more successful; which could lead to waste and cost reduction. A theoretical gap has been identified in the field of logistics in applying lean and green concepts in the context of Industry 4.0. A comprehensive literature review was conducted to address the identified research gap with the objective of examining the important lean practices and green concepts which are expected to enhance the operational performance of logistics functions. A key word based search, analysis of the topic and abstract, full text review were the steps followed respectively, for selecting the most relevant research papers which have been proven as valid, accepted and published to extract the knowledge for this study. As the major contribution, authors have developed a conceptual framework which focuses on the enhancement of operational performance of logistics operations by applying lean and green concepts with special reference to Industry 4.0 technologies. The results of the study will be beneficial for the LSP as it will suggest the strategies, concepts and techniques to enhance the operational performance of logistics functions."
c,232,2018,"The large volume of data whether arranged or unarranged is termed Big Data. Over the last decade, predictive analysis on big data has seen a high development and will absolutely keep on developing because of the rise of new interactive technology and applications because of the quick development in the field of data administrations. This paper examines diverse manners by which predictive analysis on big data can be utilized to enhance the proficiency and productiveness of government. Based on case studies, several countries like the Caribbean and Latin America made the use of big data. This paper covers perspectives, for example, government's action, administrative structures, preparing, analysing, and interpreting data."
d,233,2018,"CRISPR has become a hot research area ever since its advent for its efficiency and specificity in editing almost every section of DNA sequences. Based on its function of gene perturbation, a variety of gene editing techniques have been developed to achieve different aims. The target locations of a DNA strain can be precisely broken and repaired, during which the genes can be removed, repaired, silenced, or activated. The high efficiency of CRISPR/Cas9 reagents preparation and the easiness of experiment conduction make it now a powerful tool of high-content screen. In a CRISPR mediated screen, the reagents targeting different genes can be transferred and cultured together as a pool or separately in arrayed microwells. With the development of new synthesis methods which deceases the cost of guide RNA library preparation, arrayed CRIPSR screen is becoming more feasible and is believed to play more important role in a varies of biological research areas for its gene editing specificity and variable phenotype measurements. So far, there are a number of experiments designed in arrayed strategy, however, to the best of our knowledge, there is not a review on these experiments and their data analysis methods. Thus, we designed this systematic review and performed meta-analysis for these studies. In this paper we review the research area, study design, method of transfer of guide RNAs, final measurement of readouts and data analysis methods."
a,234,2020,"Due to the lack of living space and the increase in population, there has been a construction boom in the underground space to improve the quality of human life. Tunnel engineering plays a vital role in the development of underground space. In addition to traditional methods, some intelligent methods such as artificial neural networks (ANNs) have been applied to various problems in the tunnel domain in recent years. This paper systematically reviews the application of ANNs from different aspects of tunnel engineering. It reveals that the backpropagation algorithm (BPA) and Levenberg-Marquardt algorithm (LMA) are the most widely used. Due to the limitations of some original models, some scholars use optimization algorithms such as particle swarm optimization (PSO) and genetic algorithm (GA) to optimize the original ANNs to obtain better prediction results. A comparison between the ANN-based methods and methods like statistical methods is conducted. Finally, the following conclusions can be drawn: (1) The recommended ratio of the training set and test set is 3:1; (2) The advantage of optimized ANNs is not apparent when the optimization algorithm varies. Additionally, the performance of ANNs is always better than that of statistical methods."
b,235,2018,"Basic hardware comprehension of an artificial neural network (ANN), to a major scale depends on the proficient realization of a distinct neuron. For hardware execution of NNs, mostly FPGA-designed reconfigurable computing systems are favorable. FPGA comprehension of ANNs through a huge amount of neurons is mainly an exigent assignment. This work converses the reviews on various research articles of neural networks whose concerns focused in execution of more than one input neuron and multilayer with or without linearity property by using FPGA. An execution technique through reserve substitution is projected to adjust signed decimal facts. A detailed review of many research papers have been done for the proposed work. The proposed paper involves a Multi Layer Perceptron with a Back Propagation learning algorithm to identify a prototype for the diagnosis. In this paper, a brief introduction about artificial neural network used nowadays for diagnosis of disease is given."
c,236,2019,"The issues that relate with the automatics and intelligent tracing of the transaction and banking customer become the main issues nowadays. Whereas most of transaction that already moved to digital and online transaction that needed more intelligent features on the Predictive Analyst. The rise of predictive analytics (PA) is one of the biggest disruptions in financial services. In the last years, PA methods grew sharply to give the baseline of data driven decision-making process for forecasting future business situation. PA uses various algorithms to discover different patterns in the big data environment that might create more value for businesses including in financial institutions. The potential implementation of data science in financial industry is still to be explored. In this paper, we summarize work done on PA at financial industry. We also explore the potential application of PA and investigating how data science and big data is going to be used in financial institutions in the future."
d,237,2020,"Autism Spectrum Disorder (ASD) is a neurodevelopmental disorder that may lead to significant impairment in social communication, repetitive patterns of behavior, and possible fixed and restricted interests. Applied Behavior Analysis (ABA) is a well-supported and evidence-based treatment for the delays attributed to ASD. Assistive technologies, such as gamification, software apps, computer-based training (Web), and robotics; provide a standardized method of implementing ABA techniques. This review provides a synthesis of the main characteristics of these technologies. The assessed proposals focused on technologies such as Distributed Systems, Image Processing, Gamification, and Robotics. The primary objectives of these tools sought to improve social behavior, attention, communication, and reading skills. Some common limitations found in the literature was a failure to accurately define their target audience, and a failure to comply with the dimensions of ABA as defined by Baer, Wolf, and Risley in 1968."
a,238,2019,"This research category full paper brings a landscape of the primary studies which aim to turn diagrams accessible for blind people. There are a considerable number of people with visual impairments. However, was noticed poor accessibility features in tools to work with diagrams, which are typically used to present concepts and structured data, especially for courses in STEM fields. Thus, this systematic literature review gathered the studies about this matter in the last six years, to reveal the approaches being used to make diagrams accessible for blind people, checking, in addition to perception, if interaction and collaboration have been considered. The initial search and screening resulted in 369 studies, out of which 26 were selected for the final analysis. The results presented the preponderance of the auditory approach followed by refreshable Braille displays to achieve perception. Concerning interaction, the keyboard was the main input interface, whereas four out of nine studies found considered it. From the four studies which treated the possibility of simultaneous users working on the diagrammatic representation, only one considered distributed collaboration. Finally, the conclusion is that may there is a research gap in the collaboration, which could guide further developments."
b,239,2017,"A sentiment analysis has received a lot of attention from researchers working in the fields of natural language processing and text mining. However, there is a lack of annotated data sets that can be used to train a model for all domains, which is hampering the accuracy of sentiment analysis. Many research studies have attempted to tackle this issue and to improve cross-domain sentiment classification. In this paper, we present the results of a comprehensive systematic literature review of the methods and techniques employed in a cross-domain sentiment analysis. We focus on studies published during the period of 2010-2016. From our analysis of those works, it is clear that there is no perfect solution. Hence, one of the aims of this review is to create a resource in the form of an overview of the techniques, methods, and approaches that have been used to attempt to solve the problem of cross-domain sentiment analysis in order to assist researchers in developing new and more accurate techniques in the future."
c,240,2017,"A gene expression analysis has received a lot of interest from researchers operational in the bioinformatics fields. The expansion of bioinformatics has made it possible to study the microarray datasets straight as an alternative of to be dependent on intricate biological experiments. Usually microarray dataset's nature is to have less number of samples and large number of genes. The former referred as number of rows and later is referred as number of columns in regular transactional datasets. Inferring these microarray data residues a demanding problem and stay as forefront research part for biomedical researchers in the recent years. Emerging technologies of knowledge discovery and algorithms are being applied on gene expression to find out genomic information most related to a diversity of phenotypes. On the other hand, numerous limitations obstruct biologists from identifying and understanding important patterns, from classifying the phenotype of disease, they also struggle to realize the allegations of a particular outcome. This paper aims on a comparative study of Gene Expression Classification techniques which can classify the phenotype of diseases. In this paper, we propose an effective solution that presents concise mining results by applying classification algorithms on gene expression datasets."
d,241,2020,"Feature selection has gained much consideration from scholars working in the domain of machine learning and data mining in recent years. Feature selection is a popular problem in Machine learning with the goal of finding optimal features with increase accuracy. As a result, several studies have been conducted on multi-objective feature selection through numerous multi-objective techniques and algorithms. The objective of this paper is to present a systematic literature review of the challenges and issues of the multi-objective feature selection problem and critically analyses the proposed techniques used to tackle this problem. The conducted review covered all related studies published since 2012 up to 2019. The outcomes of the reviewed of these studies clearly showed that no perfect solution to the multi-objective feature selection problem yet. The authors believed that the conducted review would serve as the main source of the techniques and methods used to resolve the problem of multi-objective feature selection. Furthermore, current challenges and issues are deliberated to find promising research domains for further study."
a,242,2018,"Effective communication of disaster risks is crucial to provoking appropriate responses from citizens and emergency operators. With recent advancement in Artificial Intelligence (AI), several researchers have begun exploring machine learning techniques in improving disaster risk communication. This paper adopts a systematic literature approach to report on the various research activities involving the application of AI in disaster risk communication. The study found that research activities focus on two broad areas: (1) prediction and monitoring for early warning, and (2) information extraction and classification for situational awareness. These broad areas are discussed, including background information to help establish future applications of AI in disaster risk communication. The paper concludes with recommendations of several ways in which AI applications can have a broader role in disaster risk communication."
b,243,2020,"This paper presents a systematic review of relevant primary studies on the use of augmented reality (AR) to improve various skills of children and adolescents diagnosed with autism spectrum disorder (ASD) from years 2005 to 2018 inclusive in eight bibliographic databases. This systematic review attempts to address eleven specific research questions related to the learing skills, participants, AR technology, research design, data collection methods, settings, evaluation parameters, intervention outcomes, generalization, and maintenance. The social communication skill was the highly targeted skill, and individuals with ASD were part of all the studies. Computer, smartphone, and smartglass are more frequently used technologies. The commonly used research design was pre-test and post-test. Almost all the studies used observation as a data collection method, and classroom environment or controlled research environment were used as a setting of evaluation. Most of the evaluation parameters were human-assisted. The results of the studies show that AR benefited children with ASD in learning skills. The generalization test was conducted in one study only, but the results were not reported. The results of maintenance tests conducted in five studies during a short-term period following the withdrawal of intervention were positive. Although the effect of using AR towards the learning of individuals was positive, given the wide variety of skills targeted in the studies, and the heterogeneity of the participants, a summative conclusion regarding the effectiveness of AR for teaching or learning of skills related to ASD based on the existing literature is not possible. The review also proposes the research taxonomy for ASD. Future research addressing the effectiveness of AR among more participants, different technologies supporting AR for the intervention, generalization, and maintenance of learning skills, and the evaluation in the inslusive classroom environment and other settings is warranted."
c,244,2019,"Automatic depression assessment based on visual cues is a rapidly growing research domain. The present exhaustive review of existing approaches as reported in over sixty publications during the last ten years focuses on image processing and machine learning algorithms. Visual manifestations of depression, various procedures used for data collection, and existing datasets are summarized. The review outlines methods and algorithms for visual feature extraction, dimensionality reduction, decision methods for classification and regression approaches, as well as different fusion strategies. A quantitative meta-analysis of reported results, relying on performance metrics robust to chance, is included, identifying general trends and key unresolved issues to be considered in future studies of automatic depression assessment utilizing visual cues alone or in combination with vocal or verbal cues."
d,245,2020,"This article presents a systematic review of the current computational technologies applied to medical images for the detection, segmentation, and classification of strokes. Besides, analyzing and evaluating the technological advances, the challenges to be overcome and the future trends are discussed. The principal approaches make use of artificial intelligence, digital image processing and analysis, and various other technologies to develop computer-aided diagnosis (CAD) systems to improve the accuracy in the diagnostic process, as well as the interpretation consistency of medical images. However, there are some points that require greater attention such as low sensitivity, optimization of the algorithm, a reduction of false positives, and improvement in the identification and segmentation processes of different sizes and shapes. Also, there is a need to improve the classification steps of different stroke types and subtypes. Furthermore, there is an additional need for further research to improve the current techniques and develop new algorithms to overcome disadvantages identified here. The main focus of this research is to analyze the applied technologies for the development of CAD systems and verify how effective they are for stroke detection, segmentation, and classification. The main contributions of this review are that it analyzes only up-to-date studies, mainly from 2015 to 2018, as well as organizing the various studies in the area according to the research proposal, i.e., detection, segmentation, and classification of the types of stroke and the respective techniques used. Thus, the review has great relevance for future research, since it presents an ample comparison of the most recent works in the area, clearly showing the existing difficulties and the models that have been proposed to overcome such difficulties."
a,246,2019,"Question classification is a key point in many applications, such as Question Answering (QA, e.g., Yahoo! Answers), Information Retrieval (IR, e.g., Google search engine), and E-learning systems (e.g., Bloom's tax. classifiers). This paper aims to carry out a systematic review of the literature on automatic question classifiers and the technology directly involved. Automatic classifiers are responsible for labeling a certain evaluation item using a type of categorization as a selection criterion. The analysis of 80 primary studies previously selected revealed that SVM is the main algorithm of the Machine Learning used, while BOW and TF-IDF are the main techniques for feature extraction and selection, respectively. According to the analysis, the taxonomies proposed by Li and Roth and Bloom were the most used ones for the classification criteria, and Accuracy/Precision/Recall/F1-score were proven to be the most used metrics. In the future, the objective is to perform a meta-analysis with the studies that authorize the availability of their data."
b,247,2019,"Investigators spend a good amount of time detecting and searching for relevant scientific articles manually. The importance of this article is reflected in the use of best practices provided by a systematic review, which allows to obtain more relevant studies for research. The main motivation to carry out the project was the low existence of tools that support the systematic review, since there are only studies of how one is carried out. Therefore, this article proposes a tool that automates the planning phase of a systematic review. The validation of the tool allows to verify the satisfaction of the researcher according to the time and quality of the studies identified in the search for literature revision."
c,248,2017,"To improve the quality of an academic process, Higher Education Institution (HEI) becomes aware of using IT to support academic process. IT planning needs carefully plans and embed with organizational strategy to create value. Implementing IT Governance ensured that organization gain benefits from the investment in IT. This study proposed to seek benefits that HEI can get when implementing IT Governance. Literature study divided into three steps start with the review published in the various journal on the topic of IT Governance. From 105 papers found synthesized into 18 papers which relevant case in HEI and result from 11 papers to be reviewed. These selected paper then review to answer research question about the benefits of implementing IT Governance in HEI."
d,249,2018,"Purpose: To overview and characterize the potential adoption of big data analytics (BDA) in warehouse management (WM). Methodology: The paper uses a systematic review to describe the budding area of big data analytics in warehouse management, provide the benefits, summarize an architectural framework and methodology, give and present examples from industrial case studies reviewed in the literature, briefly argue the challenges, and provide conclusions. Findings: The paper offers a wide overview of big data analytics for warehouse management researchers and experts. Originality/value: Big data analytics in warehouse management is promoting into a promising domain for supplying insight from large data sets and enhancing results while decreasing costs. However, there exist challenges to conquer."
a,250,2018,"The smart city generated rapidly huge of data. Data can analyze with big data analytics technology to give solution from past data in smart city problem and help better solution in decision making. This paper summarizes the existing condition of big data analytics in the smart city in term of the algorithm, data type and tools were built using systematic literature review (SLR) as the standard methodology used to solve any problem by tracing the result from the previous research. The problem in SLR called as research question (RQ). To achieve that goal, we define some RQs related to that scope and clarify each question by tracing previous research. The research paper from reputable journal databases such as IEEE Xplore, Scopus, ScienceDirect, and Springerlink. After synthesizing 15 articles, the results are: algorithm data mining like ANN, Markov, graph mining, etc. needs to improve. That algorithm not enough to handle high data volume, high variety and high velocity to store and processes data; main data type have big chance to give the solution in the smart city is social media. That data has the potential to help in decision making in the smart city problem; Hadoop is the top tool to store and analyze data with high-performance, stable, reliable computing for the different type of data. Combination Hadoop with spark give less overhead to handle the high velocity and volume of data."
b,251,2020,"The advent of healthcare information management systems ( HIMSs ) continues to produce large volumes of healthcare data for patient care and compliance and regulatory requirements at a global scale. Analysis of this big data allows for boundless potential outcomes for discovering knowledge. Big data analytics ( BDA ) in healthcare can, for instance, help determine causes of diseases, generate effective diagnoses, enhance QoS guarantees by increasing efficiency of the healthcare delivery and effectiveness and viability of treatments, generate accurate predictions of readmissions, enhance clinical care, and pinpoint opportunities for cost savings. However, BDA implementations in any domain are generally complicated and resource-intensive with a high failure rate and no roadmap or success strategies to guide the practitioners. In this paper, we present a comprehensive roadmap to derive insights from BDA in the healthcare ( patient care ) domain, based on the results of a systematic literature review. We initially determine big data characteristics for healthcare and then review BDA applications to healthcare in academic research focusing particularly on NoSQL databases. We also identify the limitations and challenges of these applications and justify the potential of NoSQL databases to address these challenges and further enhance BDA healthcare research. We then propose and describe a state-of-the-art BDA architecture called Med-BDA for healthcare domain which solves all current BDA challenges and is based on the latest zeta big data paradigm. We also present success strategies to ensure the working of Med-BDA along with outlining the major benefits of BDA applications to healthcare. Finally, we compare our work with other related literature reviews across twelve hallmark features to justify the novelty and importance of our work. The aforementioned contributions of our work are collectively unique and clearly present a roadmap for clinical administrators, practitioners and professionals to successfully implement BDA initiatives in their organizations."
